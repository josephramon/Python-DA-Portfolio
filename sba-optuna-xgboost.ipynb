{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;a:link{color: white}\">\n",
    "    <h1 style='color:GhostWhite;'>Part 2: Should This Loan be Approved or Denied ?</h1>\n",
    "    This is a continuation of notebook <a style=\"color:yellow\" href=\"https://www.kaggle.com/code/josephramon/sba-xgboost-model\">Part 1: Should This Loan Be Approved or Denied ?</a><br><br>\n",
    "    The topic covered here is :<br>\n",
    "    <p style=\"color:Gold;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>XGBoost v1.6+ HyperParameter Tuning using Optuna - Full and Incremental</b></p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "    <b>Dataset Source</b><br><br>\n",
    "    <a href=\"https://www.kaggle.com/mirbektoktogaraev/should-this-loan-be-approved-or-denied\">U.S. Small Business Administration (SBA) Dataset</a>\n",
    "<br><br>\n",
    "    All information about the dataset can be found at the <b>above link</b><br><br>    \n",
    "    *<i>Thanks to Hamza for his <a href=\"https://www.kaggle.com/code/hamzaghanmi/xgboost-hyperparameter-tuning-using-optuna/notebook\">Notebook on Optuna</a> which was used as a guide.</i> \n",
    "<br><br>\n",
    "    If interested, Data Exploratory Visualization in Tableau can also be seen at :<br>\n",
    "    <a href= \"https://public.tableau.com/app/profile/joseph8038/viz/SBADatasetVisualizationandAnalysis/SBADatasetVisualizationandAnalysis-StoryBoard\">SBA Data Exploratory Visualization in Tableau</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"color:DarkSlateBlue\">\n",
    "This notebook is divided into 2 main parts:<br>\n",
    "<ul>\n",
    "<li><a style=\"color:DarkSlateGrey;\" href=\"#part1\"><b>Part 1: XGBoost HyperParameter Tuning using Optuna - Full and Incremental</b></a></li><br>\n",
    "<li><a style=\"color:DarkSlateGrey;\" href=\"#part2\"><b>Part 2: Miscellaneous</a></b>  - Early Stopping Rounds, Random Forest Classifier</li>\n",
    "</ul>\n",
    "<br>\n",
    "    <p style=\"color:FireBrick;\"><b>* Output from <a style=\"color:DarkGoldenRod;\" href = \"https://www.kaggle.com/code/josephramon/sba-xgboost-model\">Part 1 notebook</a> are Input to this notebook.</b></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table Of Contents</h2>\n",
    "<ul>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#libraries\">Libraries</a></li>   \n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#paths_and_flags\">Paths and Flags</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#functions\">Custom Functions And Classes</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#metrics\">Metrics Function</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#xgboost_class\">XGBoost Class</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#other_models\">Other Models Class</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#optuna_class\">Optuna Class - for both full datasets or incremental</a></li>\n",
    "    <br>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#part1\">Part 1. XGBoost HyperParameter Tuning using Optuna</a></li>\n",
    "    <ul>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#full_df\">Optuna Study : Full Dataset</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#incremental_df\">Optuna Study : Incremental Dataset</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#optuna_metrics\">Optuna Study Metrics</a></li>    \n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#try_best_hp\">Model v4 : Try the Optuna Hyperparameters</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#optuna_comparison\">Optuna Tuning Comparison</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#cross_validation\">Cross Validation</a></li>\n",
    "    </ul>\n",
    "    <br>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#part2\">Part 2. Miscellaneous</a></li>\n",
    "    <ul>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#early_stopping_rounds\">Early Stopping Rounds</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#random_forest_classifier\">Random Forest Classifier</a></li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"paths_and_flags\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 5px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Paths and Flags</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T06:49:35.140717Z",
     "iopub.status.busy": "2022-03-26T06:49:35.140141Z",
     "iopub.status.idle": "2022-03-26T06:49:35.153238Z",
     "shell.execute_reply": "2022-03-26T06:49:35.152367Z",
     "shell.execute_reply.started": "2022-03-26T06:49:35.140613Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Change this kaggle_flag to :\n",
    "   0 - if running outside Kaggle (e.g. Jupyter Notebook), change filepath & savepath to your \n",
    "       own path\n",
    "   1 - if running as a Kaggle notebook\n",
    "'''\n",
    "kaggle_flag = 0\n",
    "\n",
    "# alert_flag - change to 0 for no sound alert, 1 for sound alert after long running cells\n",
    "alert_flag = 0\n",
    "\n",
    "'''\n",
    "We have two options for running Optuna tuning on XGBoost:  \n",
    "   OptunaStudy() - run Optuna on the full dataset\n",
    "   OptunaStudyChunk() - run in chunks, lighter on memory, but much slower\n",
    "\n",
    "Change flag below as needed:\n",
    "   1 to run OptunaStudy() only\n",
    "   2 to run OptunaStudyChunk() only\n",
    "   3 to run both\n",
    "'''\n",
    "optuna_flag = 1\n",
    "\n",
    "# GPU is automatically detected if activated\n",
    "\n",
    "#---------------------------------------------------------------------------------------#\n",
    "\n",
    "if kaggle_flag == 1:             # Kaggle\n",
    "    filepath  = \"../input/sba-xgboost-model/\"\n",
    "    savepath  = \"./\"\n",
    "    final_ds  = '../input/sba-xgboost-model/sba_final.csv.feather'  # imported from Part 1 Notebook\n",
    "    final_csv = '../input/sba-xgboost-model/sba_final.csv'          # imported from Part 1 Notebook\n",
    "    functions_path = \"../usr/lib/myfuncs/myfuncs.py\"\n",
    "else:\n",
    "    filepath  = \"C:\\\\Python\\\\Python_Data_Science_Exercises\\\\datasets\\\\\"\n",
    "    savepath  = \"C:\\\\Python\\\\Python_Data_Science_Exercises\\\\datasets\\\\\"\n",
    "    final_ds  = f'{savepath}sba_final.csv.feather'\n",
    "    final_csv = f'{savepath}sba_final.csv'\n",
    "    functions_path = 'C:\\\\Python\\\\Python_Data_Science_Exercises\\\\mylibs\\\\'\n",
    "\n",
    "audio_path=\"https://www.soundjay.com/misc/sounds/tablet-bottle-1.mp3\" # for alert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"libraries\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 5px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Libraries</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package installations completed\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output   # to be able to use clear_output(wait=True)\n",
    "def install_packages():\n",
    "    print('Please wait, package installations started, if needed')\n",
    "    libs = ['scikit-learn', 'seaborn', 'numpy','matplotlib', 'tensorflow','torch','joblib',\n",
    "            'psutil','imbalanced-learn','xgboost','optuna','pyarrow','pyttsx3',\n",
    "            'pympler','memory_profiler','line_profiler']\n",
    "    \n",
    "    piplist = !pip list\n",
    "    for i in range(len(libs)):\n",
    "        if not piplist.grep(libs[i]):\n",
    "            !pip3 install {libs[i]}\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print('Package installations completed')\n",
    "\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: xgboost\n",
      "Version: 1.6.0\n",
      "Summary: XGBoost Python Package\n",
      "Home-page: https://github.com/dmlc/xgboost\n",
      "Author: \n",
      "Author-email: \n",
      "License: Apache-2.0\n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: numpy, scipy\n",
      "Required-by: autoxgb\n"
     ]
    }
   ],
   "source": [
    "# XGBoost version should be 1.6 and up\n",
    "!pip3 show xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T06:49:37.352654Z",
     "iopub.status.busy": "2022-03-26T06:49:37.352409Z",
     "iopub.status.idle": "2022-03-26T06:49:39.053576Z",
     "shell.execute_reply": "2022-03-26T06:49:39.052631Z",
     "shell.execute_reply.started": "2022-03-26T06:49:37.352624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package imports completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pyttsx3\n",
    "from IPython.display import Audio, display\n",
    "from IPython.display import FileLink\n",
    "from IPython.display import IFrame\n",
    "from IPython.core.display import HTML\n",
    "import hashlib\n",
    "import copy                     # for deepcopy()\n",
    "import datetime as dt\n",
    "import optuna\n",
    "import gc\n",
    "import shutil\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "import torch                    # for clearing GPU cache\n",
    "from time import sleep\n",
    "import multiprocessing as mp\n",
    "from pympler import muppy       # for memory profiling\n",
    "from pympler import summary     # for memory profiling\n",
    "from pympler.classtracker import ClassTracker\n",
    "import xgboost\n",
    "%load_ext line_profiler\n",
    "%load_ext memory_profiler\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "clear_output(wait=True)\n",
    "print('Package imports completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure garbage collector is enabled\n",
    "(gc.isenabled() == False) and gc.enable();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"functions\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 5px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Custom Functions and Classes</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom functions import completed\n"
     ]
    }
   ],
   "source": [
    "# import custom functions\n",
    "if functions_path not in sys.path:\n",
    "    sys.path.append(functions_path)\n",
    "from myfuncs import *\n",
    "from myfuncs import color   # eg. {color.bold}\n",
    "\n",
    "print('Custom functions import completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Custom functions and classes in <a style=\"color:ForestGreen\" href=\"https://www.kaggle.com/code/josephramon/myfuncs\" target=\"_blank\">myfuncs.py</a></b>.<br>  \n",
    "In Kaggle, myfuncs.py is set up as a <b>Utility Script</b> in /usr/lib<br>\n",
    "<ul>\n",
    "    <li>is_kaggle_gpu_enabled()</li>\n",
    "<li>clear_gpu(tree_method='gpu_hist')</li>\n",
    "<li>reduce_mem_usage(df, print_info = True, use_float16=False)</li>\n",
    "<li>runtime(rt1,rt2)</li>\n",
    "<li>create_download_link(title = \"Download \", filename = \"data.csv\")</li>\n",
    "<li>GetRam()</li>\n",
    "<li>convertFloatToDecimal(f=0.0, precision=2)</li>\n",
    "<li>formatFileSize(size, sizeIn, sizeOut, precision=0)</li>\n",
    "<li>check_cols_with_nulls(df)</li>\n",
    "<li>check_infinity_nan(df, dfname)</li>\n",
    "<li>fixvals(val)</li>\n",
    "<li>model_eval(y_valid,predictions, cmDisplay='False')</li>\n",
    "<li>plot_features(booster, figsize)</li>\n",
    "<li>make_mi_scores(X, y)</li>\n",
    "<li>plot_mi_scores(scores)</li>\n",
    "<li>GetSweetVizReport(df, savepath, kaggle_flag)</li>\n",
    "<li>class color\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hist'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_enabled = is_kaggle_gpu_enabled()\n",
    "clear_output(wait=True)\n",
    "if gpu_enabled == False:\n",
    "    tree_method = 'hist'\n",
    "else:\n",
    "    tree_method = 'gpu_hist'\n",
    "\n",
    "del gpu_enabled\n",
    "gc.collect()\n",
    "\n",
    "tree_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T06:49:39.055357Z",
     "iopub.status.busy": "2022-03-26T06:49:39.055059Z",
     "iopub.status.idle": "2022-03-26T06:49:39.063894Z",
     "shell.execute_reply": "2022-03-26T06:49:39.062913Z",
     "shell.execute_reply.started": "2022-03-26T06:49:39.055318Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3779585778.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [21]\u001b[1;36m\u001b[0m\n\u001b[1;33m    get_ipython().run_line_magic('reset_selective', '-f pyttsx3')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "b''' \n",
    "Set up voice object.  Used in different areas of notebook to indicate completion of long processes.\n",
    "'''\n",
    "if kaggle_flag == 0:   # not Kaggle\n",
    "    engine = pyttsx3.init()  # object creation\n",
    "\n",
    "    \"\"\" RATE\"\"\"\n",
    "    #rate = engine.getProperty('rate')   # getting details of current speaking rate\n",
    "    #print (rate)                        #printing current voice rate\n",
    "    engine.setProperty('rate', 175)     # setting up new voice rate\n",
    "\n",
    "    \"\"\"VOLUME\"\"\"\n",
    "    #volume = engine.getProperty('volume')   #getting to know current volume level (min=0 and max=1)\n",
    "    #print (volume)                         #printing current volume level\n",
    "    engine.setProperty('volume',0.7)        # setting up volume level  between 0 and 1\n",
    "\n",
    "    \"\"\"VOICE\"\"\"\n",
    "    voices = engine.getProperty('voices')       #getting details of current voice\n",
    "    #engine.setProperty('voice', voices[0].id)  #changing index, changes voices. o for male\n",
    "    engine.setProperty('voice', voices[1].id)   #changing index, changes voices. 1 for female\n",
    "else:b\n",
    "    %reset_selective -f pyttsx3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"xgboost_class\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 5px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>XGBoost Class</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T06:49:39.211021Z",
     "iopub.status.busy": "2022-03-26T06:49:39.210498Z",
     "iopub.status.idle": "2022-03-26T06:49:39.264543Z",
     "shell.execute_reply": "2022-03-26T06:49:39.263710Z",
     "shell.execute_reply.started": "2022-03-26T06:49:39.210977Z"
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "class process_model():  \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train, self.y_train = None, None\n",
    "        self.X_valid, self.X_test = None, None\n",
    "        self.y_valid, self.y_test = None, None\n",
    "\n",
    "        print(f'MIS_Status Count ->  1 : {Counter(y)[1]}, 0 : {Counter(y)[0]}')\n",
    "    \n",
    "    # oversampling method\n",
    "    def osample(self, print_info = True):\n",
    "        # define oversampling strategy\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority') \n",
    "        if print_info == True:\n",
    "            print('X size : ', len(self.X))\n",
    "            print('y size : ', len(self.y))\n",
    "        # fit and apply the transform\n",
    "        X_over, y_over = oversample.fit_resample(self.X, self.y)\n",
    "\n",
    "        # summarize class distribution\n",
    "        if print_info == True:\n",
    "            print(f'Before Oversampling -> 1 : {Counter(self.y)[1]}, 0 : {Counter(self.y)[0]}')\n",
    "            print(f'After Oversampling  -> 1 : {Counter(y_over)[1]}, 0 : {Counter(y_over)[0]}')\n",
    "        \n",
    "        # update X and y with the oversampled results \n",
    "        self.X = X_over\n",
    "        self.y = y_over\n",
    "        \n",
    "        # return the oversampled results in case they are needed in another module\n",
    "        #return {'X_over':X_over, 'y_over':y_over}\n",
    "    \n",
    "    def split_data(self, X_size = 0.7):   \n",
    "        # Split Data into Train:Validate:Test\n",
    "        \n",
    "        # train_size=X_size\n",
    "        # In the first step, we will split the data in training and remaining dataset\n",
    "        self.X_train, X_rem, self.y_train, y_rem = train_test_split(self.X, self.y,\n",
    "                                                        train_size = X_size, random_state=48) \n",
    "\n",
    "        # Now since we want the valid and test size to be equal,\n",
    "        # we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "        # test_size = 0.5\n",
    "\n",
    "        self.X_valid, self.X_test, self.y_valid, self.y_test = train_test_split(X_rem,y_rem,\n",
    "                                                            test_size=0.5, random_state=48)\n",
    "        \n",
    "        return {'X_train':self.X_train, 'y_train':self.y_train,\n",
    "                'X_valid':self.X_valid, 'y_valid':self.y_valid,\n",
    "                'X_test':self.X_test, 'y_test':self.y_test}\n",
    "    \n",
    "    # Method to run model \n",
    "    # desc - description of metrics report\n",
    "    def prep_run_model(self, desc='Metrics', cmDisplay=False, PipeLine_flag = False,\n",
    "                hyperparams = {'n_estimators': 1000, 'learning_rate': 0.05, 'max_depth': 6,\n",
    "                               'tree_method':tree_method, 'early_stopping_rounds':100,\n",
    "                               'eval_metric':['error', 'logloss']}):\n",
    "        # from XGBoost 1.6, early_stopping_rounds and eval_metric are under parameters,\n",
    "        # and deprecated from fit() method.\n",
    "        # The default hyperparameters are conservative, to help avoid overfitting\n",
    "        \n",
    "        print()\n",
    "        print(f\"{color.bold}Please wait, Fitting model can take time ...{color.end}\")\n",
    "        \n",
    "        '''\n",
    "        XGBRegressor is for continuous target/outcome variables. These are often called \n",
    "        \"regression problems.\"\n",
    "\n",
    "        XGBClassifier is for categorical target/outcome variables. These are often called \n",
    "        \"classification problems.\"\n",
    "        \n",
    "        xg_model = XGBRegressor(n_estimators = self.mn_estimators,\n",
    "                                learning_rate = self.mlearning_rate,\n",
    "                                max_depth = self.mmax_depth,\n",
    "                                n_jobs=4)\n",
    "        \n",
    "        xg_model = XGBClassifier(n_estimators = self.mn_estimators,\n",
    "                                learning_rate = self.mlearning_rate,\n",
    "                                max_depth = self.mmax_depth,\n",
    "                                use_label_encoder =False,\n",
    "                                n_jobs=4)\n",
    "        '''\n",
    "        \n",
    "        if PipeLine_flag == True:\n",
    "            # hyperparams is a result of Optuna hyperparameter tuning (Part 3 of this notebook)\n",
    "            # the hyperparameters lean towards being conservative to help avoid overfitting\n",
    "            hyperparams = { 'tree_method': 'gpu_hist',\n",
    "                            'lambda': 0.023437933789759252,\n",
    "                            'alpha': 0.005813454622750776,\n",
    "                            'gamma': 0,\n",
    "                            'colsample_bytree': 0.9,\n",
    "                            'subsample': 1.0,\n",
    "                            'learning_rate': 0.05,\n",
    "                            'n_estimators': 1000,\n",
    "                            'max_depth': 13,\n",
    "                            'random_state': 48,\n",
    "                            'min_child_weight': 1,\n",
    "                            'early_stopping_rounds': 100.0,\n",
    "                            'eval_metric': 'error'\n",
    "                          }\n",
    "            \n",
    "        xg_model = XGBClassifier(**hyperparams,use_label_encoder =False)\n",
    "       \n",
    "        eval_setparam = [(self.X_train, self.y_train), (self.X_valid, self.y_valid)]\n",
    "        \n",
    "        xg_model.fit(self.X_train, self.y_train, \n",
    "                     #eval_set=[(X_valid, y_valid)], \n",
    "                     eval_set = eval_setparam,\n",
    "                     verbose=False)\n",
    "        \n",
    "        gc.collect()\n",
    "        clear_gpu()\n",
    " \n",
    "        print(\"Fitting model completed.\")\n",
    "        print()\n",
    "        print('Preparing Predictions')\n",
    "    \n",
    "        # Get predictions\n",
    "        predictions = xg_model.predict(self.X_valid)\n",
    "    \n",
    "        print()\n",
    "        print(f'{color.underline}{desc}{color.end}')\n",
    "\n",
    "        eval_results = model_eval(self.y_valid, predictions, cmDisplay)\n",
    "            \n",
    "        # Return these values as they will be needed for further testing or metrics\n",
    "        # in dictionary form to remember easier \n",
    "        return {'xg_model':xg_model,'predictions':predictions,\n",
    "                    'X_train':self.X_train, 'y_train':self.y_train,\n",
    "                    'X_valid':self.X_valid, 'y_valid':self.y_valid,\n",
    "                    'X_test':self.X_test, 'y_test':self.y_test, 'eval_results':eval_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"other_models\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 5px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Other Models Class</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# inherit from XGBoost class (process_model)\n",
    "class other_models(process_model):  \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train, self.y_train = None, None\n",
    "        self.X_valid, self.X_test = None, None\n",
    "        self.y_valid, self.y_test = None, None\n",
    "\n",
    "    #    print(f'MIS_Status Count ->  1 : {Counter(y)[1]}, 0 : {Counter(y)[0]}')\n",
    "    \n",
    "    # Method to run model \n",
    "    # desc - description of metrics report\n",
    "    def prep_run_model(self, desc='Metrics', modelname = 'rfc',\n",
    "                       hparams = {'n_estimators':600, 'random_state':48, 'max_depth':10},\n",
    "                       cmDisplay=False):\n",
    "        print()\n",
    "        print(f\"{color.bold}Please wait, Fitting model can take time ...{color.end}\")  \n",
    "\n",
    "        if modelname == 'rfc':\n",
    "            model = RandomForestClassifier(**hparams) \n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            \n",
    "        print(\"Fitting model completed.\")\n",
    "        print()\n",
    "        print('Preparing Predictions')\n",
    "    \n",
    "        # Get predictions\n",
    "        predictions = model.predict(self.X_valid)\n",
    "    \n",
    "        print()\n",
    "        print(f'{color.underline}{desc}{color.end}')\n",
    "\n",
    "        eval_results = model_eval(self.y_valid, predictions, cmDisplay)\n",
    "        \n",
    "        gc.collect()\n",
    "        clear_gpu()\n",
    "        \n",
    "        # Return these values as they will be needed for further testing or metrics\n",
    "        # in dictionary form to remember easier \n",
    "        return {'model':model,'predictions':predictions,\n",
    "                    'X_train':self.X_train, 'y_train':self.y_train,\n",
    "                    'X_valid':self.X_valid, 'y_valid':self.y_valid,\n",
    "                    'X_test':self.X_test, 'y_test':self.y_test, 'eval_results':eval_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optuna_class\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Optuna Class</b><br>\n",
    "This is for both full dataset or incremental dataset trials.  There are two objective functions here,objective() and objective_batch()</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T06:49:39.268170Z",
     "iopub.status.busy": "2022-03-26T06:49:39.267940Z",
     "iopub.status.idle": "2022-03-26T06:49:39.284793Z",
     "shell.execute_reply": "2022-03-26T06:49:39.283935Z",
     "shell.execute_reply.started": "2022-03-26T06:49:39.268144Z"
    }
   },
   "outputs": [],
   "source": [
    "class optuna_tuning(process_model):  \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train, self.y_train = None, None\n",
    "        self.X_valid, self.X_test = None, None\n",
    "        self.y_valid, self.y_test = None, None\n",
    "    \n",
    "    # for printing only the best values, saves memory too\n",
    "    def logging_callback(self, study, frozen_trial):\n",
    "        previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
    "        if previous_best_value != study.best_value:\n",
    "            study.set_user_attr(\"previous_best_value\", study.best_value)\n",
    "            clear_output(wait=True)\n",
    "            self.text_out=\"{}Trial {} done with best value: {}{}{} and parameters: {}{}. \".format(\n",
    "                color.bdblue,\n",
    "                frozen_trial.number,\n",
    "                color.bdgreen,\n",
    "                frozen_trial.value,\n",
    "                color.bdblue,\n",
    "                color.end,\n",
    "                frozen_trial.params\n",
    "                )\n",
    "            print(self.text_out)\n",
    "            \n",
    "            # Writing to file\n",
    "            with open(f\"{savepath}xgb_optuna_study_log.txt\", \"a\") as os_log:\n",
    "                os_log.write('\\n\\n')\n",
    "                os_log.write(f\"Current Ram Used: {GetRam()} %\\n\")\n",
    "                os_log.write(self.text_out)\n",
    "    \n",
    "    # save study\n",
    "    def save_study(self, study, frozen_trial):\n",
    "        joblib.dump(study, f\"{savepath}xgb_optuna_study_callbacks.pkl\")   # save study\n",
    "        #print(f'{color.bdblue}Current study saved for Trial {frozen_trial.number}{color.end}')\n",
    "    \n",
    "    # for tuning full dataset\n",
    "    def objective(self, trial, gt, n_estimators = 1000):        \n",
    "        nn_early_stopping_rounds = n_estimators * 0.1\n",
    "        # the hyperparameters lean towards being conservative to help avoid overfitting\n",
    "        param = {\n",
    "            # tree_method would ideally be gpu_hist for faster speed\n",
    "            'tree_method':trial.suggest_categorical('tree_method', [tree_method]), \n",
    "            # L2 regularization weight, Increasing this value will make model more conservative\n",
    "            'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "            # L1 regularization weight, Increasing this value will make model more conservative\n",
    "            'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "            # Min loss reduction for further partition on a leaf node. larger,the more conservative\n",
    "            'gamma':trial.suggest_categorical('gamma', [0,3,6]),\n",
    "            # sampling according to each tree\n",
    "            'colsample_bytree': trial.suggest_categorical('colsample_bytree',\n",
    "                            [0.6,0.7,0.8,0.9,1.0]),\n",
    "            #                [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "            # sampling ratio for training data\n",
    "            #'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "            'subsample': trial.suggest_categorical('subsample', [0.6,0.7,0.8,0.9,1.0]),\n",
    "            'learning_rate': trial.suggest_categorical('learning_rate',\n",
    "                            [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02,0.05]),\n",
    "            'n_estimators': trial.suggest_categorical('n_estimators',[n_estimators]),\n",
    "            # maximum depth of the tree, signifies complexity of the tree\n",
    "            'max_depth': trial.suggest_categorical('max_depth', [9,11,13]),\n",
    "            'random_state': trial.suggest_categorical('random_state', [48]),\n",
    "            # minimum child weight, larger the term more conservative the tree\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'early_stopping_rounds': trial.suggest_categorical('early_stopping_rounds',\n",
    "                                          [nn_early_stopping_rounds]),                     \n",
    "            'eval_metric': trial.suggest_categorical('eval_metric', ['error','logloss'])\n",
    "        }\n",
    "        if GetRam() >= 90:\n",
    "            raise MemoryError('Short On Memory')\n",
    "            \n",
    "        # clear outputs, reprint saved data\n",
    "        if trial.number > 0:\n",
    "            clear_output(wait=True)\n",
    "            gc.collect()\n",
    "            print(self.text_out)\n",
    "            \n",
    "        # print(param)  # for debugging, comment out if desired\n",
    "        model_xgbc = XGBClassifier(**param, use_label_encoder =False)  \n",
    "    \n",
    "        print()\n",
    "        print(f\"Current Ram Used: {GetRam()} %\")\n",
    "        rt2=dt.datetime.now()\n",
    "        print(f'{color.bdgreen}Total Elapsed Time from Training Start: {color.end}', end='')\n",
    "        runtime(gt, rt2)  \n",
    "        print(f'Running Trial {trial.number}')\n",
    "            \n",
    "        model_xgbc.fit(self.X_train, self.y_train, eval_set=[(self.X_valid, self.y_valid)],\n",
    "                    verbose=False)\n",
    "\n",
    "        preds = model_xgbc.predict(self.X_valid)\n",
    "    \n",
    "        rmse = metrics.mean_squared_error(self.y_valid, preds,squared=False)\n",
    " \n",
    "        trial.report(rmse, 1)\n",
    "            \n",
    "        if trial.should_prune():\n",
    "            text_prune = f'{color.bold}Trial {trial.number} pruned{color.end}'\n",
    "            # Writing to file\n",
    "            with open(f\"{savepath}xgb_optuna_study_log.txt\", \"a\") as os_log:\n",
    "                os_log.write('\\n')\n",
    "                os_log.write(text_prune)\n",
    "            del model_xgbc, preds, text_prune\n",
    "            gc.collect()\n",
    "            sleep(3)\n",
    "            clear_gpu()\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        text_dtl = \"Trial {} finished with parameters: {}. \".format(\n",
    "            trial.number,\n",
    "            trial.params\n",
    "            )\n",
    "        # Writing to file\n",
    "        with open(f\"{savepath}xgb_optuna_study_log.txt\", \"a\") as os_log:\n",
    "            os_log.write('\\n\\n')\n",
    "            os_log.write(f\"Current Ram Used: {GetRam()} %\\n\")\n",
    "            os_log.write(text_dtl)\n",
    "            \n",
    "        rt2=dt.datetime.now()\n",
    "\n",
    "        del model_xgbc, preds, text_dtl\n",
    "        gc.collect()\n",
    "        sleep(3)       \n",
    "        clear_gpu()        \n",
    "\n",
    "        return rmse\n",
    " \n",
    "\n",
    "    # for tuning incrementally in chunks\n",
    "    def objective_chunk(self, trial, n_trials, gt,\n",
    "                        n_chunksize = 200000, n_estimators = 1000):\n",
    "        nn_early_stopping_rounds = n_estimators * 0.1\n",
    "        \n",
    "        # the hyperparameters lean towards being conservative to help avoid overfitting\n",
    "        param = {\n",
    "            # tree_method would ideally be gpu_hist for faster speed\n",
    "            'tree_method':trial.suggest_categorical('tree_method', [tree_method]), \n",
    "            # L2 regularization weight, Increasing this value will make model more conservative\n",
    "            'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "            # L1 regularization weight, Increasing this value will make model more conservative\n",
    "            'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "            # Min loss reduction for further partition on a leaf node. larger,the more conservative\n",
    "            'gamma':trial.suggest_categorical('gamma', [0,3,6]),\n",
    "            # sampling according to each tree\n",
    "            'colsample_bytree': trial.suggest_categorical('colsample_bytree',\n",
    "                                [0.6,0.7,0.8,0.9,1.0]),\n",
    "            #                [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "            # sampling ratio for training data\n",
    "            #'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "            'subsample': trial.suggest_categorical('subsample', [0.6,0.7,0.8,0.9,1.0]),\n",
    "            'learning_rate': trial.suggest_categorical('learning_rate',\n",
    "                            [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02,0.05]),\n",
    "            'n_estimators': trial.suggest_categorical('n_estimators',[n_estimators]),\n",
    "            # maximum depth of the tree, signifies complexity of the tree\n",
    "            'max_depth': trial.suggest_categorical('max_depth', [9,11,13]),\n",
    "            'random_state': trial.suggest_categorical('random_state', [48]),\n",
    "            # minimum child weight, larger the term more conservative the tree\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'early_stopping_rounds': trial.suggest_categorical('early_stopping_rounds',\n",
    "                                          [nn_early_stopping_rounds]),                     \n",
    "            'eval_metric': trial.suggest_categorical('eval_metric', ['error','logloss'])\n",
    "        }\n",
    "        if GetRam() >= 90:\n",
    "            raise MemoryError('Short On Memory')\n",
    "        \n",
    "        model_xgbc = XGBClassifier(**param,use_label_encoder =False)  \n",
    "    \n",
    "        rt1=dt.datetime.now()\n",
    "               \n",
    "        # clear outputs, reprint saved data\n",
    "        if trial.number > 0:\n",
    "            clear_output(wait=True)\n",
    "            gc.collect()\n",
    "            print(self.text_out)\n",
    "            \n",
    "            if os.path.exists(f'{savepath}model_xgbc.json'):\n",
    "                file_size = os.path.getsize(f'{savepath}model_xgbc.json')\n",
    "                file_size = formatFileSize(file_size,'B','MB',2)\n",
    "                print(f\"Current Model File Size : {file_size}MB\")\n",
    "\n",
    "        '''\n",
    "        For batch, use xgb_model parameter in fit().  There are two ways :\n",
    "           1. save the model to a file, after 1st trial, then give the name to the next trials\n",
    "           2. just give the name of the model object, in this case model_xgbc\n",
    "        '''\n",
    "    \n",
    "        #X_valid_list, y_valid_list = [],[]\n",
    "        # Fit Model\n",
    "        for i, self.X in enumerate(pd.read_csv(final_csv, chunksize = n_chunksize), start = 1):\n",
    "            self.X = reduce_mem_usage(self.X, print_info=False)\n",
    "            self.y = self.X.pop('MIS_Status')\n",
    "\n",
    "            self.osample(print_info = False)\n",
    "            self.split_data(0.7)\n",
    "            \n",
    "            #X_valid_list.append(self.X_valid.copy())\n",
    "            #y_valid_list.append(self.y_valid.copy())\n",
    "            \n",
    "            #X_valid = pd.concat(X_valid_list)\n",
    "            #y_valid = pd.concat(y_valid_list)\n",
    "            X_valid = self.X_valid\n",
    "            y_valid = self.y_valid\n",
    "        \n",
    "            self.X, self.y = None, None\n",
    "            self.X_valid, self.y_valid = None, None\n",
    "            self.X_test, self.y_test = None, None\n",
    "\n",
    "            if i == 1:            \n",
    "                print()\n",
    "                print(f\"Current Ram Used: {GetRam()} %\")\n",
    "                rt2=dt.datetime.now()\n",
    "                print(f'{color.bdgreen}Total Elapsed Time from Training Start: {color.end}', end='')\n",
    "                runtime(gt, rt2)  \n",
    "                print(f'Running Trial {trial.number} Chunk: {i}',end = ' | ')\n",
    "                model_xgbc.fit(self.X_train, self.y_train, eval_set=[(X_valid, y_valid)],\n",
    "                        verbose=False)\n",
    "            else:\n",
    "                print(f'{i}',end = ' | ')\n",
    "                model_xgbc = XGBClassifier(use_label_encoder =False)\n",
    "                model_xgbc.load_model(f'{savepath}model_xgbc.json')\n",
    "                \n",
    "                #if i == 2:\n",
    "                #    param.pop('tree_method')\n",
    "                #    param.update({'updater':'refresh',\n",
    "                #                'process_type': 'update',\n",
    "                #                'refresh_leaf': True})\n",
    "                #    model_xgbc.set_params(**param)\n",
    "                \n",
    "                model_xgbc.fit(self.X_train, self.y_train, eval_set=[(X_valid, y_valid)],\n",
    "                        verbose=False, xgb_model = model_xgbc\n",
    "                        # uncomment below if you want to use a saved file\n",
    "                        #xgb_model = f'{savepath}model_xgbc.json'\n",
    "                        )\n",
    "\n",
    "            '''Auxiliary attributes of the Python Booster object (such as feature_names) will \n",
    "            not be saved when using binary format. To save those attributes, use JSON instead.'''\n",
    "            # uncomment below if using a saved file\n",
    "            #model_xgbc.get_booster().save_model(f'{savepath}model_xgbc.json')\n",
    "            model_xgbc.save_model(f'{savepath}model_xgbc.json')\n",
    "        \n",
    "            #X_valid = pd.concat(X_valid_list)\n",
    "            #y_valid = pd.concat(y_valid_list)\n",
    "        \n",
    "            preds = model_xgbc.predict(X_valid)\n",
    "    \n",
    "            rmse = metrics.mean_squared_error(y_valid, preds,squared=False)\n",
    "        \n",
    "            del model_xgbc\n",
    "            self.X_train, self.y_train = None, None\n",
    "            gc.collect()\n",
    "            sleep(5)\n",
    "            clear_gpu()\n",
    "            \n",
    "            trial.report(rmse, i)\n",
    "            \n",
    "            if trial.should_prune():\n",
    "                # Writing to file\n",
    "                with open(f\"{savepath}xgb_optuna_study_log.txt\", \"a\") as os_log:\n",
    "                    os_log.write('\\n')\n",
    "                    os_log.write(f'{color.bold}Trial {trial.number} pruned{color.end}')\n",
    "                    os_log.write('\\n')\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        if os.path.exists(f'{savepath}model_xgbc.json'):\n",
    "            file_size = os.path.getsize(f'{savepath}model_xgbc.json')\n",
    "            file_size = formatFileSize(file_size,'B','MB',2)\n",
    "            print(f\"Current Model File Size : {file_size}MB\")\n",
    "                \n",
    "        # Writing to file\n",
    "        with open(f\"{savepath}xgb_optuna_study_log.txt\", \"a\") as os_log:\n",
    "            os_log.write('\\n\\n')\n",
    "            os_log.write(f\"Current Ram Used: {GetRam()} %\")\n",
    "            if os.path.exists(f'{savepath}model_xgbc.json'):\n",
    "                os_log.write(f\" | Current Model File Size : {file_size}MB\\n\")\n",
    "            os_log.write(f\"Trial {trial.number} finished with parameters: {trial.params}.\")\n",
    "            \n",
    "        rt2=dt.datetime.now()\n",
    "\n",
    "        self.X_train, self.y_train = None, None\n",
    "        gc.collect()\n",
    "        sleep(5)\n",
    "        clear_gpu()\n",
    "            \n",
    "        return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkRed;color:AliceBlue;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "<h1 style='color:GhostWhite;'>Part 1. XGBoost HyperParameter Tuning using Optuna</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"full_df\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>1.1 Optuna Study - Full Dataset</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "        \n",
    "# For running Optuna tuning on full dataset.\n",
    "def OptunaStudy():\n",
    "    X = pd.read_feather(final_ds)\n",
    "    y = X.pop('MIS_Status')\n",
    "    \n",
    "    # instantiate the optuna_tuning class\n",
    "    ot = optuna_tuning(X, y)\n",
    "    ot.osample()\n",
    "    ot.split_data(0.7)\n",
    "    \n",
    "    ot.X, ot.y = None, None\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "    sleep(3)\n",
    "    \n",
    "    ''' \n",
    "    - Initialize the number of trials and n_estimators.\n",
    "    - Rather than run at a very high n_trials like 100 and above, I prefer to run this function, \n",
    "         with an n_trials like 50 or 70, three times to get the best_trial values \n",
    "    - Recommended n_estimators is 100 for really huge datasets, 1000 for medium-sized.\n",
    "    - For our dataset here, 1000 is fine.  Ramping it up to 4000 for example will have no\n",
    "         significant benefits, will be very slow, and a Kaggle notebook will run out of memory often.\n",
    "    '''\n",
    "    \n",
    "    nn_trials = 3\n",
    "    nn_estimators = 1000\n",
    "    \n",
    "    if os.path.exists(f'{savepath}xgb_optuna_study_log.txt'):\n",
    "        os.remove(f'{savepath}xgb_optuna_study_log.txt')\n",
    "\n",
    "    # STUDY\n",
    "    \n",
    "    # Turn off optuna log notes, to use own logging notes\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARN)\n",
    "\n",
    "    print(f\"{color.bold}Please wait, finding best trial ...{color.end}\")\n",
    "            \n",
    "    gt = dt.datetime.now()\n",
    "        \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    try:\n",
    "        # callbacks [self.save_study] is to save study in case memory fails\n",
    "        study.optimize(lambda trial: ot.objective(trial, gt, n_estimators = nn_estimators), \n",
    "                        n_trials = nn_trials,\n",
    "                        callbacks = [ot.logging_callback, ot.save_study],\n",
    "                        gc_after_trial = True,\n",
    "                        catch = (RuntimeWarning,ArithmeticError,))\n",
    "    except MemoryError as e:\n",
    "        print(f'{color.bdblue}{e} : Memory was getting low, Trial ended early{color.end}')\n",
    "    \n",
    "    print()\n",
    "    print(f'Number of completed trials: {color.bdgreen}{len(study.trials)}{color.end}')\n",
    "    print(f'{color.bold}Best trial: {study.best_trial.params}{color.end}')\n",
    "    \n",
    "    joblib.dump(study, f\"{savepath}xgb_optuna_study.pkl\")   # save study\n",
    "    # jl = joblib.load(f\"{savepath}xgb_optuna_study.pkl\")   # load study\n",
    "\n",
    "    print()\n",
    "    return study\n",
    "\n",
    "if optuna_flag == 1 or optuna_flag == 3:\n",
    "    study_results = OptunaStudy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"incremental_df\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>1.2 Optuna Study - Incremental Dataset</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T07:06:56.335865Z",
     "iopub.status.busy": "2022-03-26T07:06:56.335510Z",
     "iopub.status.idle": "2022-03-26T08:03:41.820703Z",
     "shell.execute_reply": "2022-03-26T08:03:41.819946Z",
     "shell.execute_reply.started": "2022-03-26T07:06:56.335827Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# For running Optuna tuning incrementally in batches, much slower, but lighter on memory\n",
    "def OptunaStudyChunk(): \n",
    "    ''' \n",
    "    - Initialize the number of trials and n_estimators.\n",
    "    - Rather than run at a very high n_trials like 100 and above, I prefer to run this function, \n",
    "         with an n_trials like 50 or 70, three times to get the best_trial values \n",
    "    - Recommended n_estimators is 100 for really huge datasets, 1000 for medium-sized.\n",
    "    - For our dataset here, 1000 is fine.  Ramping it up to 4000 for example will have no\n",
    "         significant benefits, will be very slow, and a Kaggle notebook will run out of memory often.\n",
    "    '''\n",
    "\n",
    "    nn_trials = 3               # n_trials\n",
    "    nn_chunksize = 200000\n",
    "    nn_estimators = 1000\n",
    "    \n",
    "    if os.path.exists(f'{savepath}xgb_optuna_study_log.txt'):\n",
    "        os.remove(f'{savepath}xgb_optuna_study_log.txt')\n",
    "\n",
    "    # get full X_valid, y_valid \n",
    "    '''\n",
    "    print(f\"{color.bold}Please wait, getting Validation Data{color.end}\")  \n",
    "    X_valid_list, y_valid_list = [],[]\n",
    "    for i, X in enumerate(pd.read_csv(final_csv, chunksize = nn_chunksize), start = 1):\n",
    "        X = reduce_mem_usage(X, print_info=False)\n",
    "        y = X.pop('MIS_Status')\n",
    "\n",
    "        oso = optuna_tuning(X,y) \n",
    "        oso.osample(print_info = False)\n",
    "        oso.split_data(0.7)\n",
    "            \n",
    "        X_valid_list.append(oso.X_valid.copy())\n",
    "        y_valid_list.append(oso.y_valid.copy())\n",
    "            \n",
    "    X_valid = pd.concat(X_valid_list)\n",
    "    y_valid = pd.concat(y_valid_list)\n",
    "        \n",
    "    del X_valid_list, y_valid_list, oso\n",
    "    gc.collect() \n",
    "    sleep(3)\n",
    "    '''\n",
    "        \n",
    "    # OPTUNA STUDY\n",
    "    \n",
    "    # Turn off optuna log notes, to use own logging notes\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARN)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"{color.bold}Please wait, finding best trial ...{color.end}\")\n",
    "            \n",
    "    gt = dt.datetime.now()\n",
    "\n",
    "    otb = optuna_tuning(None, None)\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    try:\n",
    "        # callbacks [self.save_study] is to save study in case memory fails\n",
    "        '''\n",
    "        study.optimize(lambda trial: otb.objective_chunk(trial, nn_trials, gt, X_valid, y_valid,\n",
    "                                  n_chunksize = nn_chunksize, n_estimators = nn_estimators), \n",
    "                        n_trials = nn_trials,\n",
    "                        callbacks = [otb.logging_callback, otb.save_study],\n",
    "                        gc_after_trial = True,\n",
    "                        catch = (RuntimeWarning,ArithmeticError,))\n",
    "        '''\n",
    "        study.optimize(lambda trial: otb.objective_chunk(trial, nn_trials, gt,\n",
    "                                  n_chunksize = nn_chunksize, n_estimators = nn_estimators), \n",
    "                        n_trials = nn_trials,\n",
    "                        callbacks = [otb.logging_callback, otb.save_study],\n",
    "                        gc_after_trial = True,\n",
    "                        catch = (RuntimeWarning,ArithmeticError,))\n",
    "    except MemoryError as e:\n",
    "        print(f'\\n\\n{color.bdblue}{e} : Memory was getting low, Trial ended early{color.end}\\n\\n')\n",
    "            \n",
    "    print(f'Number of completed trials: {color.bdgreen}{len(study.trials)}{color.end}')\n",
    "    print(f'{color.bold}Best trial: {study.best_trial.params}{color.end}')\n",
    "    \n",
    "    joblib.dump(study, f\"{savepath}xgb_optuna_study.pkl\")   # save study\n",
    "    # jl = joblib.load(f\"{savepath}xgb_optuna_study.pkl\")   # load study\n",
    "\n",
    "    print()\n",
    "    return study\n",
    "\n",
    "#if optuna_flag == 2 or optuna_flag == 3:\n",
    "study_results = OptunaStudyChunk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optuna_metrics\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>1.3 Optuna Study Metrics</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below if you want to see the trials log \n",
    "with open(f\"{savepath}xgb_optuna_study_log.txt\", \"r+\") as os_log:\n",
    "    print(os_log.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below if you want to load saved study to check, if desired\n",
    "jl = joblib.load(f\"{savepath}xgb_optuna_study.pkl\")\n",
    "jl = jl.best_trial.params\n",
    "pprint(jl)\n",
    "# jl.update({'n_estimators': 4000, 'tree_method':tree_method})\n",
    "'''\n",
    "best_trial_sorted = {}\n",
    "for i in sorted (jl) :\n",
    "    best_trial_sorted.update({i:jl[i]})                          \n",
    "\n",
    "pprint(best_trial_sorted)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_results = joblib.load(f\"{savepath}xgb_optuna_study.pkl\")\n",
    "best_trial = study_results.best_trial.params\n",
    "best_trial['tree_method'] = tree_method\n",
    "pprint(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = study_results.best_trial.params\n",
    "#best_trial.update({'n_estimators': 4000, 'tree_method':tree_method})\n",
    "best_trial['tree_method'] = tree_method\n",
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T08:03:41.834515Z",
     "iopub.status.busy": "2022-03-26T08:03:41.834060Z",
     "iopub.status.idle": "2022-03-26T08:03:41.859859Z",
     "shell.execute_reply": "2022-03-26T08:03:41.859192Z",
     "shell.execute_reply.started": "2022-03-26T08:03:41.834477Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trial results dataframe sorted from best value (RMSE) ascending\n",
    "def ViewResultsAsDf():\n",
    "    stdf = study_results.trials_dataframe()\n",
    "    stdf = stdf.sort_values('value',ascending=True)\n",
    "\n",
    "    return stdf.head(2)    # return here is only used for printing output\n",
    "\n",
    "ViewResultsAsDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T08:03:41.861308Z",
     "iopub.status.busy": "2022-03-26T08:03:41.860969Z",
     "iopub.status.idle": "2022-03-26T08:03:44.062901Z",
     "shell.execute_reply": "2022-03-26T08:03:44.062232Z",
     "shell.execute_reply.started": "2022-03-26T08:03:41.861270Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visualize parameter importance\n",
    "optuna.visualization.plot_param_importances(study_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T08:03:44.064544Z",
     "iopub.status.busy": "2022-03-26T08:03:44.064101Z",
     "iopub.status.idle": "2022-03-26T08:03:44.071602Z",
     "shell.execute_reply": "2022-03-26T08:03:44.070934Z",
     "shell.execute_reply.started": "2022-03-26T08:03:44.064505Z"
    }
   },
   "outputs": [],
   "source": [
    "if alert_flag == 1:\n",
    "    if kaggle_flag == 0:   # not Kaggle\n",
    "        engine.say(\"Optuna run completed.\")\n",
    "        engine.runAndWait()\n",
    "    else:\n",
    "        display(Audio(url=audio_path, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"try_best_hp\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>1.4 Model v4 : Try the Optuna Hyperparameters</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T08:03:44.073183Z",
     "iopub.status.busy": "2022-03-26T08:03:44.072800Z",
     "iopub.status.idle": "2022-03-26T08:07:48.555184Z",
     "shell.execute_reply": "2022-03-26T08:07:48.554423Z",
     "shell.execute_reply.started": "2022-03-26T08:03:44.073143Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def RunModelv4():\n",
    "    X = pd.read_feather(final_ds)\n",
    "    y = X.pop('MIS_Status')\n",
    "\n",
    "    model4 = process_model(X, y)\n",
    "    model4.osample()\n",
    "    model4.split_data(0.7)\n",
    "    \n",
    "    model4.X, model4.y = None, None\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "    sleep(3)\n",
    "    \n",
    "    model4_results = model4.prep_run_model(\"Metrics : After Optuna Tuning\",\n",
    "                                           hyperparams = best_trial)\n",
    "    \n",
    "    text_boosted = \\\n",
    "        f\"Total boosted rounds: {model4_results['xg_model'].get_booster().num_boosted_rounds()}\"\n",
    "    print(text_boosted)\n",
    "    \n",
    "    # save to files for reuse later\n",
    "    model4_results['xg_model'].save_model(f'{savepath}modelv4.json')\n",
    "    joblib.dump(model4_results, f\"{savepath}model4_results.dict\")   \n",
    "\n",
    "#with warnings.catch_warnings():\n",
    "#    warnings.filterwarnings('ignore')\n",
    "#    warnings.simplefilter(\"ignore\")\n",
    "RunModelv4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=0.008607607720665177, base_score=0.5, booster='gbtree',\n",
       "              callbacks=None, colsample_bylevel=1, colsample_bynode=1,\n",
       "              colsample_bytree=1.0, early_stopping_rounds=100.0,\n",
       "              enable_categorical=False, eval_metric='logloss', gamma=0,\n",
       "              gpu_id=-1, grow_policy='depthwise', importance_type=None,\n",
       "              interaction_constraints='', lambda=0.1609056174694532,\n",
       "              learning_rate=0.014, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=9, max_leaves=0, min_child_weight=2,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=1000,\n",
       "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=48, ...)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelv4 = XGBClassifier()\n",
    "modelv4.load_model(f'{savepath}modelv4.json')\n",
    "modelv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T08:07:48.556949Z",
     "iopub.status.busy": "2022-03-26T08:07:48.556435Z",
     "iopub.status.idle": "2022-03-26T08:07:48.563964Z",
     "shell.execute_reply": "2022-03-26T08:07:48.563172Z",
     "shell.execute_reply.started": "2022-03-26T08:07:48.556902Z"
    }
   },
   "outputs": [],
   "source": [
    "if alert_flag == 1:\n",
    "    if kaggle_flag == 0:   # not Kaggle\n",
    "        engine.say(\"Model Test with Optuna completed.\")\n",
    "        engine.runAndWait()\n",
    "    else:\n",
    "        display(Audio(url=audio_path, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optuna_comparison\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>1.5 Optuna Tuning Comparison</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Compare metrics before and after Optuna tuning.</b><br><br>\n",
    "    Comparison is made between modelv3 results in <a style=\"color:DarkSlateGrey\" href=\"https://www.kaggle.com/code/josephramon/sba-xgboost-model\">Part 1 Notebook</a> and modelv4 results here.\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T08:07:48.566044Z",
     "iopub.status.busy": "2022-03-26T08:07:48.565194Z",
     "iopub.status.idle": "2022-03-26T08:07:48.583368Z",
     "shell.execute_reply": "2022-03-26T08:07:48.582657Z",
     "shell.execute_reply.started": "2022-03-26T08:07:48.565947Z"
    }
   },
   "outputs": [],
   "source": [
    "def CompareResults():\n",
    "    model3_results = joblib.load(f\"{savepath}model3_results.dict\")\n",
    "    model4_results = joblib.load(f\"{savepath}model4_results.dict\")\n",
    "\n",
    "    m3_clf_report = model3_results['eval_results']['ClassificationReport']\n",
    "\n",
    "    m3_0_f1_score = round(m3_clf_report['0']['f1-score'] * 100, 2)\n",
    "    m3_1_f1_score = round(m3_clf_report['1']['f1-score'] * 100, 2)\n",
    "    m3_accuracy   = round(m3_clf_report['accuracy'] * 100, 2)\n",
    "\n",
    "\n",
    "    m4_clf_report = model4_results['eval_results']['ClassificationReport']\n",
    "\n",
    "    m4_0_f1_score = round(m4_clf_report['0']['f1-score'] * 100, 2)\n",
    "    m4_1_f1_score = round(m4_clf_report['1']['f1-score'] * 100, 2)\n",
    "    m4_accuracy   = round(m4_clf_report['accuracy'] * 100, 2)\n",
    "\n",
    "\n",
    "    data = {'Model v3 : No Optuna':[m3_0_f1_score, m3_1_f1_score, m3_accuracy],\n",
    "            'Model v4 : With Optuna':[m4_0_f1_score, m4_1_f1_score, m4_accuracy]}\n",
    " \n",
    "    # Creates pandas DataFrame.\n",
    "    df = pd.DataFrame(data, index =['0 : f1_score',\n",
    "                                    '1 : f1_score',\n",
    "                                    'Accuracy'])\n",
    "    print(f'{color.bdgreen}Accuracy Improvement Using Optuna Suggested Parameters:{color.end}')\n",
    "    print(f'{color.bold}Improved by {color.bdblue}{round(m4_accuracy - m3_accuracy,2)}\\\n",
    "        {color.end}')\n",
    "    return df\n",
    "\n",
    "CompareResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T08:07:48.585169Z",
     "iopub.status.busy": "2022-03-26T08:07:48.584709Z",
     "iopub.status.idle": "2022-03-26T08:07:48.818069Z",
     "shell.execute_reply": "2022-03-26T08:07:48.817400Z",
     "shell.execute_reply.started": "2022-03-26T08:07:48.585132Z"
    }
   },
   "outputs": [],
   "source": [
    "del best_trial, study_results\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Observation:</b><br><br>\n",
    "    <b>The Accuracy and F1 scores after Optuna tuning are improved compared to before tuning;</b> but it all depends on what hyperparameters/values are given.  A few trial sessions may be needed.<br><br>\n",
    "    We have a different score in our <a style=\"color:DarkSlateGrey\" href=\"https://www.kaggle.com/code/josephramon/sba-xgboost-model\">Pipeline</a> as we used an Optuna hyperparameter set that was obtained from another Optuna run.\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cross_validation\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>Cross Validation</h2><br>\n",
    "Measure our model's quality, in RMSE.  Ideally for small datasets, but included here for reference.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def CrossVal():\n",
    "    print(f'{color.bold}Please wait, this will take some time{color.end}')\n",
    "    print()\n",
    "    X = pd.read_feather(final_ds)\n",
    "    y = X.pop('MIS_Status')\n",
    "\n",
    "    cvo = process_model(X, y)   # create object from XGBoost class\n",
    "    cvo.osample()               # oversample\n",
    "    cvo.split_data(0.7)\n",
    "\n",
    "    cvo.X, cvo.y = None, None\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "    sleep(3)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # get hyperparams from previous Optuna study's best trial\n",
    "    study_results = joblib.load(f\"{savepath}xgb_optuna_study.pkl\")\n",
    "    hyperparams = study_results.best_trial.params\n",
    "    pprint(hyperparams)\n",
    "    print()\n",
    "\n",
    "    xgb_model = XGBClassifier(**hyperparams, use_label_encoder = False)\n",
    "\n",
    "    # If we pass a pipeline instead of a model to cross_val_score, fit_params won't be \n",
    "    # recognized\n",
    "    fit_params={'verbose': False,\n",
    "                'eval_set': [(cvo.X_valid, cvo.y_valid)]\n",
    "               }\n",
    "\n",
    "    # Multiply by -1 since sklearn calculates *negative* RMSE\n",
    "    print()\n",
    "    scores = -1 * cross_val_score(xgb_model, cvo.X_train, cvo.y_train,\n",
    "                                  cv=5,\n",
    "                                  scoring='neg_root_mean_squared_error',\n",
    "                                  fit_params = fit_params,\n",
    "                                  verbose=15)\n",
    "    sleep(5)   # allow verbosity to complete\n",
    "    print()\n",
    "    print(f\"{color.bdblue}Scores: {scores}{color.end}\")\n",
    "    print()\n",
    "    print(f\"{color.bdgreen}Root Mean Squared Error (Mean): {scores.mean()}{color.end}\")\n",
    "    print()\n",
    "    \n",
    "CrossVal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part4\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkRed;color:AliceBlue;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "<h1 id=\"part1\" style='color:GhostWhite;'>Part 4. Miscellaneous</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"early_stopping_rounds\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>4.1 Early Stopping Rounds</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Below is a reference on using plots to get an insight on the value to use for XGBoost's early_ stopping_rounds during fitting.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def PlotEarlyStoppingRounds():\n",
    "    X = pd.read_feather(final_ds)\n",
    "    y = X.pop('MIS_Status')\n",
    "    \n",
    "    esr = process_model(X, y)\n",
    "    esr.osample()     # oversample\n",
    "    esr.split_data(0.7)\n",
    "    \n",
    "    esr.X, esr.y = None, None\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "    sleep(3)   \n",
    "    \n",
    "    print()\n",
    "    print(f\"{color.bold}Please wait, Fitting model can take time ...{color.end}\")\n",
    "    print()\n",
    "    \n",
    "    # get hyperparams from previous Optuna study's best trial\n",
    "    # early_stopping_rounds not included as we are trying to confirm if our value was valid\n",
    "    study_results = joblib.load(f\"{savepath}xgb_optuna_study.pkl\")\n",
    "    hyperparams = study_results.best_trial.params\n",
    "    hyperparams.pop('early_stopping_rounds')\n",
    "    hyperparams['eval_metric'] = ['error','logloss']\n",
    "    pprint(hyperparams)\n",
    "    print()\n",
    "\n",
    "    xg_model = XGBClassifier(**hyperparams,use_label_encoder =False)\n",
    "       \n",
    "    eval_setparam = [(esr.X_train, esr.y_train),\n",
    "                     (esr.X_valid, esr.y_valid)]\n",
    "       \n",
    "    # fit the model\n",
    "    xg_model.fit(esr.X_train, esr.y_train, \n",
    "                eval_set = eval_setparam,\n",
    "                verbose=False)\n",
    " \n",
    "    print(\"Fitting model completed.\")\n",
    "    print()\n",
    "    print('Preparing Predictions')\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = xg_model.predict(esr.X_valid)\n",
    "    \n",
    "    print()\n",
    "    print(f'{color.underline}Metrics:{color.end}')\n",
    "\n",
    "    eval_results = model_eval(esr.y_valid, predictions)\n",
    "\n",
    "    # retrieve performance metrics\n",
    "    results = xg_model.evals_result()\n",
    "    epochs = len(results['validation_0']['error'])\n",
    "    x_axis = range(0, epochs)\n",
    "\n",
    "    # what we will be looking for are the bottom areas of the plots\n",
    "    \n",
    "    # plot log loss\n",
    "    fig, ax = pyplot.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['logloss'], label='Test')\n",
    "    ax.legend()\n",
    "    pyplot.xlabel('Epoch')\n",
    "    pyplot.ylabel('Log Loss')\n",
    "    pyplot.title('XGBoost Log Loss')\n",
    "    pyplot.show()\n",
    "\n",
    "    # plot classification error\n",
    "    fig, ax = pyplot.subplots()\n",
    "    ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "    ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "    ax.legend()\n",
    "    pyplot.xlabel('Epoch')\n",
    "    pyplot.ylabel('Classification Error')\n",
    "    pyplot.title('XGBoost Classification Error')\n",
    "    pyplot.show()\n",
    "    \n",
    "PlotEarlyStoppingRounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    From both plots, we can see that 10% of n_estimator is a good candidate as the early_stopping_rounds parameter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"random_forest_classifier\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>4.2 Random Forest Classifier</h2><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    This is just a reference on using a Random Forest Classifier.\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# run before tuning\n",
    "def RunModelrf():\n",
    "    X = pd.read_feather(final_ds)\n",
    "    y = X.pop('MIS_Status')\n",
    "\n",
    "    modelrf = other_models(X, y)\n",
    "    modelrf.osample()\n",
    "    modelrf.split_data(0.7)\n",
    "    \n",
    "    modelrf.X, modelrf.y = None, None\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "    sleep(3)\n",
    "    \n",
    "    modelrf.prep_run_model(\"Metrics : Random Forest Classifier\", modelname='rfc')\n",
    "\n",
    "RunModelrf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Optuna Tuning for Random Forest</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    This is just a simple sample implementation, for reference.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define an objective function to be maximized.\n",
    "def objective_rf(trial, X_train, y_train, X_valid, y_valid):\n",
    "    nn_max_depth = trial.suggest_categorical(\"max_depth\", [5,10,15])\n",
    "    nn_estimators = trial.suggest_categorical('n_estimators', [100,250,500,750,1000])\n",
    "    \n",
    "    rf_obj = RandomForestClassifier(max_depth = nn_max_depth,\n",
    "                                    n_estimators = nn_estimators\n",
    "                                    # warm_start = True            # for incremental learning\n",
    "                                   )  \n",
    "    ## Fit Model\n",
    "    rf_obj.fit(X_train, y_train)\n",
    "\n",
    "    # Report intermediate objective value\n",
    "    intermediate_value = rf_obj.score(X_valid, y_valid)\n",
    "    trial.report(intermediate_value, 0)\n",
    "     \n",
    "    del rf_obj\n",
    "    gc.collect()\n",
    "        \n",
    "    # Handle pruning based on the intermediate value.\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    \n",
    "    return intermediate_value\n",
    "\n",
    "def RandomForestOptunaTuning():\n",
    "    X = pd.read_feather(final_ds)\n",
    "    y = X.pop('MIS_Status')\n",
    "    print(X.info())\n",
    "\n",
    "    rfo = process_model(X, y)\n",
    "    rfo.osample()  # oversample\n",
    "    rfo.split_data(0.7)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    del X, y\n",
    "    gc.collect()\n",
    "    sleep(3)\n",
    "    \n",
    "    print(f'{color.bold}Please wait, this will take time{color.end}')\n",
    "    \n",
    "    # Turn oN optuna log notes\n",
    "    optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    \n",
    "    nn_trials = 20\n",
    "    try:\n",
    "        study.optimize(lambda trial: objective_rf(trial,\n",
    "                        rfo.X_train, rfo.y_train,\n",
    "                        rfo.X_valid, rfo.y_valid),\n",
    "                        n_trials = nn_trials,\n",
    "                        catch = (RuntimeWarning,ArithmeticError,))\n",
    "    except MemoryError as e:\n",
    "        print(f'{color.bdblue}{e} : Memory was getting low, Trial ended early{color.end}')\n",
    "            \n",
    "    # Calculating the pruned and completed trials\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "    \n",
    "    return study\n",
    "\n",
    "study_results = RandomForestOptunaTuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(study_results.best_params) # Get best parameters for the objective function.\n",
    "print()\n",
    "pprint(study_results.best_value)  # Get best objective value.\n",
    "print()\n",
    "pprint(study_results.best_trial)  # Get best trial's information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T08:03:41.822297Z",
     "iopub.status.busy": "2022-03-26T08:03:41.822021Z",
     "iopub.status.idle": "2022-03-26T08:03:41.832229Z",
     "shell.execute_reply": "2022-03-26T08:03:41.831237Z",
     "shell.execute_reply.started": "2022-03-26T08:03:41.822262Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_trial = study_results.best_trial.params\n",
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T08:03:41.834515Z",
     "iopub.status.busy": "2022-03-26T08:03:41.834060Z",
     "iopub.status.idle": "2022-03-26T08:03:41.859859Z",
     "shell.execute_reply": "2022-03-26T08:03:41.859192Z",
     "shell.execute_reply.started": "2022-03-26T08:03:41.834477Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trial results dataframe sorted from best value ascending\n",
    "ViewResultsAsDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-26T08:03:41.861308Z",
     "iopub.status.busy": "2022-03-26T08:03:41.860969Z",
     "iopub.status.idle": "2022-03-26T08:03:44.062901Z",
     "shell.execute_reply": "2022-03-26T08:03:44.062232Z",
     "shell.execute_reply.started": "2022-03-26T08:03:41.861270Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visualize parameter importance\n",
    "optuna.visualization.plot_param_importances(study_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Random Forest Score With Optuna Hyperparameters</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def RunModelrf2():\n",
    "    X = pd.read_feather(final_ds)\n",
    "    y = X.pop('MIS_Status')\n",
    "\n",
    "    modelrf = other_models(X, y)\n",
    "    modelrf.osample()\n",
    "    modelrf.split_data(0.7)\n",
    "    \n",
    "    modelrf.X, modelrf.y = None, None\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "    sleep(3)\n",
    "    \n",
    "    modelrf.prep_run_model(\"Metrics : Random Forest Classifier\",\n",
    "                            modelname='rfc', hparams = best_trial)\n",
    "\n",
    "RunModelrf2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Once again, Optuna helped us get parameters that improved the score.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "del study_results, best_trial\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>RandomizedSearchCV</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Below is a reference on using a <b>RandomizedSearchCV</b> first for Random Forest hyperparameter tuning.<br><br>\n",
    "  Once done, one would have randomly narrowed down some parameters which we can base our inputs for a full <b>GridSearchCV</b> (not shown here).\n",
    "    <br><br>\n",
    "    Both approaches take an <b>extremely long time to run</b> using our SBA dataset, and the line to run the task is commented out.  Uncomment if you want to try.  Otherwise, <b>Optuna</b> is a much faster method.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ViewDefaultRFCParams():\n",
    "    rf = RandomForestClassifier(random_state = 48)\n",
    "    # Look at parameters used by our current forest\n",
    "    print('Default parameters in use:\\n')\n",
    "    pprint(rf.get_params())\n",
    "\n",
    "ViewDefaultRFCParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def SuggestRFCParams():\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 500, stop = 2000, num = 3)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(6, 15, num = 4)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    \n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    pprint(random_grid)\n",
    "    return random_grid\n",
    "\n",
    "random_grid = SuggestRFCParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def RandomSearchCV(random_grid):\n",
    "    X = pd.read_feather(final_ds)\n",
    "    y = X.pop('MIS_Status')\n",
    "\n",
    "    modelrf = other_models(X, y)\n",
    "    modelrf.osample()\n",
    "    modelrf.split_data(0.7)\n",
    "    \n",
    "    X_train, y_train = modelrf.X_train, modelrf.y_train\n",
    "    \n",
    "    del X, y, modelrf\n",
    "    gc.collect()\n",
    "    sleep(3)\n",
    "    \n",
    "    # Use the random grid to search for best hyperparameters\n",
    "    # First create the base model to tune\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n",
    "                                   n_iter = 5, cv = 3, verbose=10, random_state=48)\n",
    "\n",
    "    # Fit the random search model\n",
    "    rf_random.fit(X_train, y_train)\n",
    "    \n",
    "    return rf_random.best_params_\n",
    "\n",
    "#rf_best_params = RandomSearchCV(random_grid)\n",
    "#rf_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del random_grid #,rf_best_params \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
