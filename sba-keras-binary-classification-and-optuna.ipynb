{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;a:link{color: white}\">\n",
    "    <h1 style='color:GhostWhite;'>Part 3: Should This Loan be Approved or Denied ?</h1>\n",
    "    This is Part 3 of notebook <a style=\"color:yellow\" href=\"https://www.kaggle.com/code/josephramon/sba-xgboost-model\">Part 1: Should This Loan Be Approved or Denied ?</a><br><br>\n",
    "    This notebook is divided into 3 main parts:<br>\n",
    "<ul>\n",
    "<li><a style=\"color:Gold;\" href=\"#part1\"><b>1. tf.Keras Binary Classification Model</b></a></li>\n",
    "<li><a style=\"color:Gold;\" href=\"#part2\"><b>2. Optuna Hyperparameter Tuning</b></a> </li>\n",
    "<li><a style=\"color:Gold;\" href=\"#part3\"><b>3. Keras Tuner Hyperparameter Tuning</b></a> </li>\n",
    "</ul>\n",
    "<br>\n",
    "    <i><b>Output from <a style=\"color:Gold;\" href = \"https://www.kaggle.com/code/josephramon/sba-xgboost-model\">Part 1 notebook</a> are Input to this notebook.</b></i><br><br>\n",
    "    The techniques covered here are :<br>\n",
    "    <p style=\"color:Gold;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>Deep Learning: keras Binary Classification</b></p>\n",
    "    <p style=\"color:Gold;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>keras.experimental.WideDeepModel</b></p>\n",
    "    <p style=\"color:Gold;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>Optuna Hyperparameter Tuning for Keras</b></p>\n",
    "    <p style=\"color:Gold;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>Keras Tuner Hyperparameter Tuning</b></p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">  \n",
    "    <b>Dataset Source</b><br>\n",
    "    <a href=\"https://www.kaggle.com/mirbektoktogaraev/should-this-loan-be-approved-or-denied\">U.S. Small Business Administration (SBA) Dataset</a> - all information about the dataset can be found in this link \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"color:DarkSlateBlue\">\n",
    "    Kaggle's <b>Tensor Processing Units (TPUs)</b> are accelerators ideal for large workloads.  Using a TPU is a bit like using eight GPUs at once.<br><br>\n",
    "    <b>Model Training Time Comparison in Kaggle :<br>\n",
    "        200 epochs with early stopping, and our X_train dataset shape is almost a million rows after oversampling.</b><br>\n",
    "    <li>With TPU - about 10 minutes</li>\n",
    "    <li>With GPU - about 1 hour</li>\n",
    "    <li>Without TPU or GPU - more than 24 hours</li>\n",
    "    <br>\n",
    "For running Optuna or Keras Tuner here, it would be ideal to use TPU. One can still just use GPU, but it will take about 18 to 20 hours for an Optuna 30-trial run, for example.<br><br>\n",
    "Optuna is faster than Keras Tuner Bayesian Optimization.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "<h2>Table Of Contents</h2>\n",
    "<ul>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#paths_and_flags\">Paths and Flags</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#libraries\">Libraries</a></li>   \n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#functions\">Custom Functions And Classes</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#prepdataset_class\">PrepDataset Class</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#clr_class\">Cyclical Learning Rate Class</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#keras_binary_class\">Keras Binary Classification Class</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#tp_class\">TestPredict Class</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#cr_class\">CompareResults Class</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#optuna_class\">Optuna Class</a></li>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#keras_tuner_class\">Keras Tuner Class</a></li>\n",
    "    <br>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#part1\">Part 1. tf.Keras Models</a></li>\n",
    "    <ul>\n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#binary_classification\">Binary Classification tf.keras model</a></li>\n",
    "        <ul>\n",
    "            <li><a style=\"color:DarkSlateGrey\" href=\"#test_unseen\">Test Binary Classification Model with Unseen Dataset</a></li>\n",
    "            <li><a style=\"color:DarkSlateGrey\" href=\"#uit\">Test Binary Classification Model with Single User Inputs</a></li>\n",
    "        </ul>\n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#binary_classification_wide_and_deep\">Binary Classification Wide and Deep tf.keras model</a></li>\n",
    "        <ul>\n",
    "            <li><a style=\"color:DarkSlateGrey\" href=\"#test_unseen_wd\">Test keras.experimental.WideDeepModel with Unseen Dataset</a></li>\n",
    "            <li><a style=\"color:DarkSlateGrey\" href=\"#uit_wd\">Test keras.experimental.WideDeepModel with Single User Inputs</a></li>\n",
    "        </ul>\n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#compare_bc_wd\">Score Comparison : Binary Classification vs WideAndDeep Models</a></li>\n",
    "    </ul>   \n",
    "    <br>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#part2\">Part 2. Optuna Hyperparameter Tuning</a></li>\n",
    "    <ul>\n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#optuna_study\">Optuna Study</a></li> \n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#optuna_metrics\">Optuna Study Metrics</a></li>\n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#try_best_hp\">Test Optuna Best Trial Hyperparameters</a></li>\n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#optuna_comparison\">Optuna Tuning Comparison</a></li>\n",
    "    </ul>\n",
    "    <br>\n",
    "    <li><a style=\"color:DarkSlateGrey\" href=\"#part3\">Part 3. Keras Tuner Hyperparameter Tuning</a></li>\n",
    "    <ul>\n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#kt_study\">Keras Tuner Search</a></li> \n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#kt_metrics\">Keras Tuner Metrics</a></li>\n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#try_best_hp\">Test Keras Tuner's Best Trial Hyperparameters</a></li>\n",
    "        <li><a style=\"color:DarkSlateGrey\" href=\"#kt_comparison\">Keras Tuner Tuning Comparison</a></li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"paths_and_flags\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 5px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Paths and Flags</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:18:54.132261Z",
     "iopub.status.busy": "2022-05-23T20:18:54.131789Z",
     "iopub.status.idle": "2022-05-23T20:18:54.153265Z",
     "shell.execute_reply": "2022-05-23T20:18:54.152149Z",
     "shell.execute_reply.started": "2022-05-23T20:18:54.132139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running a Kaggle notebook\n",
      "kaggle_flag : 0\n",
      "alert_flag  : 0\n",
      "optuna_flag : 1\n",
      "kt_flag     : 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "'''\n",
    "kaggle_flag :\n",
    "   0 - if running outside Kaggle (e.g. Jupyter Notebook), change inputdir & workdir to your \n",
    "       own path\n",
    "   1 - if running as a Kaggle notebook\n",
    "'''\n",
    "# Change this logic to your own if needed\n",
    "if os.path.exists('../usr/lib/myfuncs/myfuncs.py'):\n",
    "    kaggle_flag = 1\n",
    "    print('Running a Kaggle notebook')\n",
    "else:\n",
    "    kaggle_flag = 0\n",
    "    print('Not running a Kaggle notebook')\n",
    "\n",
    "# alert_flag - change to 0 for no sound alert, 1 for sound alert after long running cells\n",
    "alert_flag = 0\n",
    "\n",
    "'''\n",
    "Change flag below as needed:\n",
    "   1 to run Optuna\n",
    "   0 Do Not run Optuna\n",
    "'''\n",
    "optuna_flag = 1\n",
    "\n",
    "'''\n",
    "Change flag below as needed:\n",
    "   1 to run Keras Tuner\n",
    "   0 Do Not run Keras Tuner\n",
    "'''\n",
    "kt_flag = 0\n",
    "\n",
    "# GPU is automatically detected if activated\n",
    "\n",
    "#---------------------------------------------------------------------------------------#\n",
    "\n",
    "if kaggle_flag == 1:             # Kaggle\n",
    "    # Part 1 Notebook's workdir contents are input for this notebook\n",
    "    inputdir  = \"../input/sba-xgboost-model/\"        \n",
    "    workdir  = \"./\"\n",
    "    final_ds  = f'{inputdir}sba_final.csv.feather'  # imported from Part 1 Notebook\n",
    "    final_csv = f'{inputdir}sba_final.csv'          # imported from Part 1 Notebook\n",
    "    functions_path = \"../usr/lib/myfuncs/myfuncs.py\"\n",
    "else:\n",
    "    inputdir  = \"C:\\\\Python\\\\Python_Data_Science_Exercises\\\\datasets\\\\\"\n",
    "    workdir  = \"C:\\\\Python\\\\Python_Data_Science_Exercises\\\\datasets\\\\\"\n",
    "    final_ds  = f'{inputdir}sba_final.csv.feather'\n",
    "    final_csv = f'{inputdir}sba_final.csv'\n",
    "    functions_path = 'C:\\\\Python\\\\Python_Data_Science_Exercises\\\\mylibs\\\\'\n",
    "\n",
    "audio_path=\"https://www.soundjay.com/misc/sounds/tablet-bottle-1.mp3\" # for alert\n",
    "\n",
    "print(f'kaggle_flag : {kaggle_flag}')\n",
    "print(f'alert_flag  : {alert_flag}')\n",
    "print(f'optuna_flag : {optuna_flag}')\n",
    "print(f'kt_flag     : {kt_flag}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"libraries\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 5px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Libraries</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:18:54.155916Z",
     "iopub.status.busy": "2022-05-23T20:18:54.155134Z",
     "iopub.status.idle": "2022-05-23T20:18:55.915667Z",
     "shell.execute_reply": "2022-05-23T20:18:55.914696Z",
     "shell.execute_reply.started": "2022-05-23T20:18:54.155871Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package installations completed\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output   # to be able to use clear_output(wait=True)\n",
    "def install_packages():\n",
    "    print('Please wait, package installations started, if needed')\n",
    "    libs = ['scikit-learn', 'seaborn', 'numpy','matplotlib', 'tensorflow','torch','joblib',\n",
    "            'psutil','imbalanced-learn','pyarrow','pyttsx3','sweetviz', 'optuna', 'pydotplus',\n",
    "            'keras-tuner']\n",
    "    \n",
    "    piplist = !pip list\n",
    "    for i in range(len(libs)):\n",
    "        if not piplist.grep(libs[i]):\n",
    "            !pip3 install {libs[i]}\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    print('Package installations completed')\n",
    "\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:18:55.917856Z",
     "iopub.status.busy": "2022-05-23T20:18:55.917507Z",
     "iopub.status.idle": "2022-05-23T20:19:00.118855Z",
     "shell.execute_reply": "2022-05-23T20:19:00.118007Z",
     "shell.execute_reply.started": "2022-05-23T20:18:55.917822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package imports completed\n"
     ]
    }
   ],
   "source": [
    "print('Please wait, importing packages')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import gc\n",
    "import shutil\n",
    "import psutil\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import joblib\n",
    "import copy                     # for deepcopy()\n",
    "import datetime as dt\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyttsx3\n",
    "#import hashlib\n",
    "\n",
    "import torch                    # for clearing GPU cache\n",
    "%matplotlib inline\n",
    "\n",
    "import optuna\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.constraints import unit_norm\n",
    "\n",
    "import pydot\n",
    "import pydotplus\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras_tuner as kt\n",
    "from mock import patch\n",
    "\n",
    "# for preparing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler   #, OneHotEncoder\n",
    "#from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# for CyclicLR class\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "clear_output(wait=True)\n",
    "print('Package imports completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras-tuner version 1.1.2 is fine\n"
     ]
    }
   ],
   "source": [
    "# Kernel must be restarted if Keras Tuner is upgraded\n",
    "# importlib.reload and %autoreload do not work, so manually restart \n",
    "# This check is basically for Kaggle which has an older version of Keras Tuner, as at May 2022\n",
    "if kt.__version__ < '1.0.3':\n",
    "    !pip3 install --upgrade keras-tuner\n",
    "    clear_output(wait=True)\n",
    "    print('keras-tuner Package upgrade completed.  KERNEL RESTART NEEDED FOR NOTEBOOK.')\n",
    "else:\n",
    "    print(f'keras-tuner version {kt.__version__} is fine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras-tuner __Version__ : 1.1.2\n",
      "\n",
      "Name: keras-tuner\n",
      "Version: 1.1.2\n",
      "Summary: Hypertuner for Keras\n",
      "Home-page: https://github.com/keras-team/keras-tuner\n",
      "Author: The KerasTuner authors\n",
      "Author-email: kerastuner@google.com\n",
      "License: Apache License 2.0\n",
      "Location: c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requires: numpy, tensorboard, kt-legacy, ipython, packaging, requests\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# keras-tuner version should be 1.0.3 and up\n",
    "assert kt.__version__ >= '1.03',\\\n",
    "    \"kers-tuner version must be 1.0.3+. RESTART KERNEL if already upgraded.\"\n",
    "\n",
    "print(f'keras-tuner __Version__ : {kt.__version__}')\n",
    "print()\n",
    "!pip3 show keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:00.120675Z",
     "iopub.status.busy": "2022-05-23T20:19:00.120110Z",
     "iopub.status.idle": "2022-05-23T20:19:00.124797Z",
     "shell.execute_reply": "2022-05-23T20:19:00.123967Z",
     "shell.execute_reply.started": "2022-05-23T20:19:00.120644Z"
    }
   },
   "outputs": [],
   "source": [
    "# enable garbage collector if disabled\n",
    "(gc.isenabled() == False) and gc.enable();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"functions\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 5px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Custom Functions and Classes</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:00.157520Z",
     "iopub.status.busy": "2022-05-23T20:19:00.157260Z",
     "iopub.status.idle": "2022-05-23T20:19:00.170273Z",
     "shell.execute_reply": "2022-05-23T20:19:00.169383Z",
     "shell.execute_reply.started": "2022-05-23T20:19:00.157494Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:00.172065Z",
     "iopub.status.busy": "2022-05-23T20:19:00.171773Z",
     "iopub.status.idle": "2022-05-23T20:19:00.238014Z",
     "shell.execute_reply": "2022-05-23T20:19:00.237395Z",
     "shell.execute_reply.started": "2022-05-23T20:19:00.172032Z"
    }
   },
   "outputs": [],
   "source": [
    "# import custom functions\n",
    "# RESTART kernel if myfuncs is modified\n",
    "if functions_path not in sys.path:\n",
    "    sys.path.append(functions_path)\n",
    "from myfuncs import *\n",
    "\n",
    "print('Custom functions import completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Custom functions and classes in <a style=\"color:ForestGreen\" href=\"https://www.kaggle.com/code/josephramon/myfuncs\" target=\"_blank\">myfuncs.py</a></b>.<br>  \n",
    "In Kaggle, myfuncs.py is set up as a <b>Utility Script</b> in /usr/lib<br>\n",
    "<ul>\n",
    "    <li>is_kaggle_gpu_enabled()</li>\n",
    "<li>clear_gpu(tree_method='gpu_hist')</li>\n",
    "<li>reduce_mem_usage(df, print_info = True, use_float16=False)</li>\n",
    "<li>runtime(rt1,rt2)</li>\n",
    "<li>create_download_link(title = \"Download \", filename = \"data.csv\")</li>\n",
    "<li>GetRam()</li>\n",
    "<li>convertFloatToDecimal(f=0.0, precision=2)</li>\n",
    "<li>formatFileSize(size, sizeIn, sizeOut, precision=0)</li>\n",
    "<li>check_cols_with_nulls(df)</li>\n",
    "<li>check_infinity_nan(df, dfname)</li>\n",
    "<li>fixvals(val)</li>\n",
    "<li>model_eval(y_valid,predictions, cmDisplay='False')</li>\n",
    "<li>plot_features(booster, figsize)</li>\n",
    "<li>make_mi_scores(X, y)</li>\n",
    "<li>plot_mi_scores(scores)</li>\n",
    "<li>GetSweetVizReport(df, workdir, kaggle_flag)</li>\n",
    "<li>SetVoice(kaggle_flag)</li>\n",
    "<li>InitTPUStrategy()</li>\n",
    "<li>ZipDir(zippath)</li>\n",
    "<li>class color\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:00.239642Z",
     "iopub.status.busy": "2022-05-23T20:19:00.238918Z",
     "iopub.status.idle": "2022-05-23T20:19:05.251945Z",
     "shell.execute_reply": "2022-05-23T20:19:05.251370Z",
     "shell.execute_reply.started": "2022-05-23T20:19:00.239611Z"
    }
   },
   "outputs": [],
   "source": [
    "gpu_enabled = is_kaggle_gpu_enabled()\n",
    "\n",
    "sleep(5)\n",
    "clear_output(wait=True)\n",
    "print(\"Is GPU enabled ? \", gpu_enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:05.253032Z",
     "iopub.status.busy": "2022-05-23T20:19:05.252808Z",
     "iopub.status.idle": "2022-05-23T20:19:05.258207Z",
     "shell.execute_reply": "2022-05-23T20:19:05.257378Z",
     "shell.execute_reply.started": "2022-05-23T20:19:05.253004Z"
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Set up voice object.  Used in different areas of notebook to indicate completion of long processes.\n",
    "'''\n",
    "engine = SetVoice(kaggle_flag)\n",
    "print('Voice engine set up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepdataset_class\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 5px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>PrepDataset Class</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:05.261207Z",
     "iopub.status.busy": "2022-05-23T20:19:05.260811Z",
     "iopub.status.idle": "2022-05-23T20:19:05.287646Z",
     "shell.execute_reply": "2022-05-23T20:19:05.286729Z",
     "shell.execute_reply.started": "2022-05-23T20:19:05.261169Z"
    }
   },
   "outputs": [],
   "source": [
    "class PrepDataset:  \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train, self.y_train = None, None\n",
    "        self.X_valid, self.X_test = None, None\n",
    "        self.y_valid, self.y_test = None, None\n",
    "    \n",
    "    # oversampling method\n",
    "    def osample(self, print_info = True):\n",
    "        # define oversampling strategy\n",
    "        oversample = RandomOverSampler(sampling_strategy='minority') \n",
    "        if print_info == True:\n",
    "            print('X size : ', len(self.X))\n",
    "            print('y size : ', len(self.y))\n",
    "        # fit and apply the transform\n",
    "        X_over, y_over = oversample.fit_resample(self.X, self.y)\n",
    "\n",
    "        # summarize class distribution\n",
    "        if print_info == True:\n",
    "            print(f'y Before Oversampling -> 1 : {Counter(self.y)[1]}, 0 : {Counter(self.y)[0]}')\n",
    "            print(f'y After Oversampling  -> 1 : {Counter(y_over)[1]}, 0 : {Counter(y_over)[0]}')\n",
    "        \n",
    "        # update X and y with the oversampled results \n",
    "        self.X = X_over\n",
    "        self.y = y_over\n",
    "    \n",
    "    def split_data(self, X_size = 0.7):   \n",
    "        # Split Data into Train:Validate:Test\n",
    "        \n",
    "        # train_size=X_size\n",
    "        # In the first step, we will split the data in training and remaining dataset\n",
    "        self.X_train, X_rem, self.y_train, y_rem = train_test_split(self.X, self.y,\n",
    "                                        train_size = X_size, random_state=48) \n",
    "\n",
    "        # Now since we want the valid and test size to be equal,\n",
    "        # we have to define valid_size=0.5 (that is 50% of remaining data)\n",
    "        # test_size = 0.5\n",
    "\n",
    "        self.X_valid, self.X_test, self.y_valid, self.y_test = train_test_split(X_rem,y_rem,\n",
    "                                        test_size=0.5, random_state=48)\n",
    "        \n",
    "        return {'X_train':self.X_train, 'y_train':self.y_train,\n",
    "                'X_valid':self.X_valid, 'y_valid':self.y_valid,\n",
    "                'X_test':self.X_test, 'y_test':self.y_test}\n",
    "\n",
    "    def Scale_Data(self):\n",
    "        # Scale dataset\n",
    "        #scalar = MinMaxScaler()\n",
    "        scalar = StandardScaler()\n",
    "        scalar.fit(self.X_train)\n",
    "        self.X_train = scalar.transform(self.X_train)\n",
    "        self.X_valid = scalar.transform(self.X_valid)\n",
    "        self.X_test = scalar.transform(self.X_test)\n",
    "\n",
    "        # transform into numpy arrays\n",
    "        self.y_train, self.y_valid, self.y_test =\\\n",
    "            self.y_train.values, self.y_valid.values, self.y_test.values\n",
    "        \n",
    "    def Scale_Data_Pipeline(self):\n",
    "        # Scale dataset using a pipeline, just another approach (the other one is Scale_Data())\n",
    "        features_num = list(self.X)\n",
    "        #print(features_num)\n",
    "\n",
    "        # features_cat     # we don't have any category types\n",
    "\n",
    "        transformer_num = make_pipeline(\n",
    "            #SimpleImputer(strategy=\"constant\"), # there are a few missing values\n",
    "            StandardScaler(),\n",
    "            #MinMaxScaler(),\n",
    "            )\n",
    "        '''\n",
    "        transformer_cat = make_pipeline(\n",
    "            SimpleImputer(strategy=\"constant\", fill_value=\"NA\"),\n",
    "            OneHotEncoder(handle_unknown='ignore'),\n",
    "        )\n",
    "        '''\n",
    "        preprocessor = make_column_transformer(\n",
    "            (transformer_num, features_num),\n",
    "            #(transformer_cat, features_cat),\n",
    "        )\n",
    "    \n",
    "        # Scale Dataset\n",
    "        self.X_train = preprocessor.fit_transform(self.X_train)\n",
    "        self.X_valid = preprocessor.transform(self.X_valid)\n",
    "        self.X_test  = preprocessor.transform(self.X_test)\n",
    "\n",
    "        # convert to numpy arrays\n",
    "        self.y_train, self.y_valid, self.y_test =\\\n",
    "            self.y_train.values, self.y_valid.values, self.y_test.values\n",
    "\n",
    "    def Scale_Data_Yeo_Johnson(self):\n",
    "        features_num = list(self.X)\n",
    "\n",
    "        transformer_num = make_pipeline(\n",
    "            PowerTransformer(method = 'yeo-johnson'),\n",
    "            )\n",
    "        \n",
    "        preprocessor = make_column_transformer(\n",
    "            (transformer_num, features_num),\n",
    "        )\n",
    "        \n",
    "        # Scale Dataset\n",
    "        self.X_train = preprocessor.fit_transform(self.X_train)\n",
    "        self.X_valid = preprocessor.transform(self.X_valid)\n",
    "        self.X_test  = preprocessor.transform(self.X_test)\n",
    "\n",
    "        # convert to numpy arrays\n",
    "        self.y_train, self.y_valid, self.y_test =\\\n",
    "            self.y_train.values, self.y_valid.values, self.y_test.values\n",
    "        \n",
    "    def Scale_Data_Np_Log(self):\n",
    "        # only features that are not 0/1\n",
    "        features_num = list(self.X)\n",
    "        \n",
    "        # Scale Dataset\n",
    "        self.X_train = np.log(self.X_train+1)\n",
    "        self.X_valid = np.log(self.X_valid+1)\n",
    "        self.X_test  = np.log(self.X_test+1)\n",
    "\n",
    "        # convert to numpy arrays\n",
    "        self.y_train, self.y_valid, self.y_test =\\\n",
    "            self.y_train.values, self.y_valid.values, self.y_test.values\n",
    "        \n",
    "print('PrepDataset class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clr_class\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Cyclical Learning Rate Class</b><br>\n",
    "    <i>source: </i><a style=\"color:MidnightBlue;\" href = \"https://github.com/bckenstler/CLR/blob/master/clr_callback.py\">https://github.com/bckenstler/CLR/blob/master/clr_callback.py<a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:05.289134Z",
     "iopub.status.busy": "2022-05-23T20:19:05.288770Z",
     "iopub.status.idle": "2022-05-23T20:19:05.313843Z",
     "shell.execute_reply": "2022-05-23T20:19:05.313037Z",
     "shell.execute_reply.started": "2022-05-23T20:19:05.289107Z"
    }
   },
   "outputs": [],
   "source": [
    "class CyclicLR(callbacks.Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[callbacks.clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[callbacks.clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        #print line added by me\n",
    "        #print(f'\\n{color.bdgreen}Learning Rate: {K.eval(self.model.optimizer.lr)}{color.end}\\n')\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr +\\\n",
    "                (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr +\\\n",
    "                (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "        if self.clr_iterations == 0:\n",
    "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            K.set_value(self.model.optimizer.lr, self.clr()) \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        K.set_value(self.model.optimizer.lr, self.clr())\n",
    "        \n",
    "print('CyclicLR class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.metrics.RootMeanSquaredError(\n",
    "    name='root_mean_squared_error', dtype=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"keras_binary_class\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Keras Binary Classification Class</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:05.316364Z",
     "iopub.status.busy": "2022-05-23T20:19:05.315749Z",
     "iopub.status.idle": "2022-05-23T20:19:05.364791Z",
     "shell.execute_reply": "2022-05-23T20:19:05.363806Z",
     "shell.execute_reply.started": "2022-05-23T20:19:05.316322Z"
    }
   },
   "outputs": [],
   "source": [
    "class KerasBinaryClassification(PrepDataset):\n",
    "    def __init__(self, X, y):\n",
    "        # use class PrepDataset's __init__ variables\n",
    "        PrepDataset.__init__(self, X, y)\n",
    "        \n",
    "        #-----------------------------\n",
    "        # Initialize default values\n",
    "        #-----------------------------\n",
    "        \n",
    "        # Model Configuration\n",
    "        self.UNITS = 2 ** 11 # 2048\n",
    "        self.ACTIVATION = 'swish'\n",
    "        self.DROPOUT = 0.1\n",
    "        #self.kernel_constraint_val = 3   # for dropout\n",
    "\n",
    "        self.EPOCHS = 200\n",
    "        self.EARLY_STOPPING_ROUNDS = self.EPOCHS * 0.1   # 10%\n",
    "    \n",
    "        # Training Configuration\n",
    "        # To go fast on a TPU, increase the batch size. The rule of thumb is to use batches of 128 \n",
    "        # elements per core (ex: batch size of 128*8=1024 for a TPU with 8 cores).\n",
    "        self.BATCH_SIZE_PER_REPLICA = 2 ** 11 # powers of 128 are best\n",
    "        \n",
    "        # For model.compile\n",
    "        # To reduce Python overhead and maximize the performance of your TPU, pass in the \n",
    "        # argument steps_per_execution to model.compile. In this example, it increases throughput\n",
    "        self.steps_per_execution = 256\n",
    "        self.learning_rate = None\n",
    "        self.compile_metrics = ['AUC', 'binary_accuracy', 'Precision', 'Recall']\n",
    "        \n",
    "        self.kt_run = False          # becomes True in WideAndDeepModel(), if Keras Tuner was run\n",
    "        \n",
    "        # For model.fit\n",
    "        self.clr_triangular = None    # for CyclicLR object instantiated in PrepKSModel() below\n",
    "        self.model = None\n",
    "        #batch_size = BATCH_SIZE_PER_REPLICA * tpu_strategy.num_replicas_in_sync\n",
    "        #steps_per_epoch = len(self.X_train) // batch_size\n",
    "        #validation_steps = len(self.X_valid) // batch_size\n",
    "        \n",
    "        self.fit_callbacks = None\n",
    "        \n",
    "        # early_stopping\n",
    "        self.early_stopping, self.lr_schedule = None, None\n",
    "        \n",
    "        self.input_shape = None\n",
    "        \n",
    "        self.n_layers = 6\n",
    "        \n",
    "        # wide and deep models\n",
    "        self.wide, self.deep, self.wide_and_deep = None, None, None\n",
    "    \n",
    "    def dense_block(self, units, activation, dropout_rate, l1=None, l2=None):\n",
    "        def make(inputs):\n",
    "            x = layers.Dense(units)(inputs)\n",
    "            x = layers.Activation(activation)(x)\n",
    "            x = layers.BatchNormalization()(x)                # ideally after Activation\n",
    "            x = layers.Dropout(dropout_rate)(x)               \n",
    "            return x\n",
    "        return make\n",
    "\n",
    "    # Create a model using the Sequential method\n",
    "    def create_seq_model(self):\n",
    "        '''instantiating the model in the strategy scope creates the model on the TPU'''\n",
    "        with tpu_strategy.scope():\n",
    "            model = keras.Sequential()\n",
    "            model.add(BatchNormalization(input_shape=self.input_shape))\n",
    "            for i in range(self.n_layers):\n",
    "                model.add(Dense(self.UNITS, activation=self.ACTIVATION))\n",
    "                                #kernel_constraint=unit_norm()))\n",
    "                                #kernel_constraint=max_norm(self.kernel_constraint_val)))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(self.DROPOUT))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "            \n",
    "            self.model = model\n",
    "        \n",
    "        # Prepare Keras model (compile, callbacks for early_stopping and lr_schedule)\n",
    "        self.PrepKSModel()\n",
    "    \n",
    "        return self.model\n",
    "    \n",
    "    # create a model using the Functional API method, which is more flexible than Sequential()\n",
    "    # e.g. can have more inputs and outputs\n",
    "    def create_func_api_model(self):\n",
    "        '''instantiating the model in the strategy scope creates the model on the TPU'''\n",
    "        with tpu_strategy.scope():\n",
    "            inputs = keras.Input(shape = self.input_shape)\n",
    "        \n",
    "            for i in range(self.n_layers):\n",
    "                if i == 0:\n",
    "                    x = self.dense_block(self.UNITS, self.ACTIVATION, self.DROPOUT)(inputs)\n",
    "                else:\n",
    "                    x = self.dense_block(self.UNITS, self.ACTIVATION, self.DROPOUT)(x)\n",
    "\n",
    "            outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "            self.model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "        # Prepare Keras model (compile, callbacks for early_stopping and lr_schedule)\n",
    "        self.PrepKSModel()\n",
    "\n",
    "        return self.model\n",
    " \n",
    "    # create a wide_and_deep model using keras.experimental.WideDeepModel and Functional API\n",
    "    def create_wide_and_deep_model(self):\n",
    "        '''instantiating the model in the strategy scope creates the model on the TPU'''\n",
    "        with tpu_strategy.scope():\n",
    "            # Wide Network\n",
    "            self.wide = keras.experimental.LinearModel()\n",
    "\n",
    "            # Deep Network\n",
    "            inputs = keras.Input(shape = self.input_shape)\n",
    "            for i in range(self.n_layers):\n",
    "                if i == 0:\n",
    "                    x = self.dense_block(self.UNITS, self.ACTIVATION, self.DROPOUT)(inputs)\n",
    "                else:\n",
    "                    x = self.dense_block(self.UNITS, self.ACTIVATION, self.DROPOUT)(x)  \n",
    "            outputs = layers.Dense(1)(x)\n",
    "            self.deep = keras.Model(inputs = inputs, outputs = outputs)\n",
    "    \n",
    "            # Wide and Deep Network\n",
    "            self.wide_and_deep = keras.experimental.WideDeepModel(\n",
    "                                        linear_model = self.wide,\n",
    "                                        dnn_model = self.deep,\n",
    "                                        activation = 'sigmoid',\n",
    "                                        )\n",
    "            self.model = self.wide_and_deep\n",
    "        # Prepare Keras model (compile, callbacks for early_stopping and lr_schedule)\n",
    "        self.PrepKSModel()        \n",
    "\n",
    "        return self.model\n",
    "\n",
    "    # Prepare Keras model\n",
    "    def PrepKSModel(self):\n",
    "        '''Starting with Tensorflow 2.4, model.compile() accepts a new steps_per_execution \n",
    "        parameter. This parameter instructs Keras to send multiple batches to the TPU at once. \n",
    "        In addition to lowering communications overheads, this gives the XLA compiler the \n",
    "        opportunity to optimize TPU hardware utilization across multiple batches. \n",
    "    \n",
    "        For classification, what we want instead is a distance between probabilities, and this is \n",
    "        what cross-entropy provides. Cross-entropy is a sort of measure for the distance from one \n",
    "        probability distribution to another.  For two-class problems, be sure to use 'binary' \n",
    "        versions.\n",
    "    \n",
    "        The cross-entropy and accuracy functions both require probabilities as inputs, meaning, \n",
    "        numbers from 0 to 1. To covert the real-valued outputs produced by a dense layer into \n",
    "        probabilities, we attach a new kind of activation function, the sigmoid activation.\n",
    "        \n",
    "\n",
    "        To get the final class prediction, we define a threshold probability. Typically this \n",
    "        will be 0.5, so that rounding will give us the correct class: below 0.5 means the class \n",
    "        with label 0, and 0.5 or above means the class with label 1. A 0.5 threshold is what Keras \n",
    "        uses by default with its accuracy metric.\n",
    "        '''\n",
    "        \n",
    "        tf.keras.metrics.BinaryAccuracy(name='binary_accuracy', dtype=None, threshold=0.5)\n",
    "        \n",
    "        if self.learning_rate == None:\n",
    "            opt = tf.keras.optimizers.Adam()\n",
    "        else:\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer = opt,\n",
    "            loss = 'binary_crossentropy',\n",
    "            metrics = self.compile_metrics,\n",
    "            steps_per_execution = self.steps_per_execution,\n",
    "        )\n",
    "\n",
    "        # Callbacks when fitting\n",
    "        '''\n",
    "        \"The learning rate is perhaps the most important hyperparameter. If you have \n",
    "        time to tune only one hyperparameter, tune the learning rate.\"\n",
    "        '''\n",
    "        self.clr_triangular = CyclicLR(mode='triangular')\n",
    "        \n",
    "        '''\n",
    "        During training, we'll use the EarlyStopping callback as usual while fitting.\n",
    "        '''\n",
    "        self.early_stopping = callbacks.EarlyStopping(\n",
    "            monitor = 'val_loss',\n",
    "            mode = 'auto',\n",
    "            patience = self.EARLY_STOPPING_ROUNDS,\n",
    "            min_delta = 0.001,\n",
    "            restore_best_weights = True,\n",
    "            verbose = 1\n",
    "        )\n",
    "        \n",
    "        # This will not be used at the moment, but instead will use the CyclicLR class\n",
    "        '''\n",
    "        We've also defined a learning rate schedule. It's been found that gradually decreasing \n",
    "        the learning rate over the course of training can improve performance (the weights \n",
    "        \"settle in\" to a minimum). This schedule will multiply the learning rate by 0.2 if the \n",
    "        validation loss didn't decrease after an epoch.\n",
    "        '''\n",
    "        self.lr_schedule = callbacks.ReduceLROnPlateau(\n",
    "            monitor = 'val_loss',\n",
    "            mode = 'auto',\n",
    "            patience = 0,\n",
    "            factor = 0.2,\n",
    "            min_lr = 0.001,\n",
    "            verbose = 1\n",
    "        )\n",
    "        \n",
    "        if self.kt_run == False:       # if Keras Tuner is not being run\n",
    "            self.fit_callbacks = [  \n",
    "                                    self.early_stopping,\n",
    "                                    self.clr_triangular\n",
    "                                 ]\n",
    "                                    #self.lr_schedule]\n",
    "        else:\n",
    "            self.fit_callbacks=[self.early_stopping]\n",
    "\n",
    "        return self.model\n",
    "    \n",
    "    def FitModel(self):\n",
    "        history = self.model.fit(\n",
    "                                 self.X_train,\n",
    "                                 self.y_train,\n",
    "                                 validation_data = (self.X_valid, self.y_valid),\n",
    "                                 batch_size = self.BATCH_SIZE_PER_REPLICA,\n",
    "                                 epochs = self.EPOCHS,\n",
    "                                 #steps_per_epoch=steps_per_epoch,\n",
    "                                 #validation_steps=validation_steps,\n",
    "                                 callbacks = self.fit_callbacks\n",
    "                                )\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def SavePlotHistory(self, hdf, savename):\n",
    "        hdf['f1_score'] = 2*((hdf['precision']*hdf['recall'])/(hdf['precision']+hdf['recall']))\n",
    "        hdf['val_f1_score'] = 2*((hdf['val_precision']*hdf['val_recall'])/\n",
    "                                 (hdf['val_precision']+hdf['val_recall']))\n",
    "    \n",
    "        print()\n",
    "        print(f'{color.bdgreen}{color.underline}SCORES:{color.end}')\n",
    "        display(hdf.tail(5)[['loss','auc','binary_accuracy','precision','recall','f1_score']])\n",
    "        display(hdf.tail(5)[['val_loss','val_auc','val_binary_accuracy',\n",
    "                     'val_precision','val_recall', 'val_f1_score']])\n",
    "    \n",
    "        # Save history\n",
    "        hdf_save = hdf.copy().reset_index(drop=True)\n",
    "        hdf_save.to_feather(f'{workdir}{savename}')\n",
    "        print()\n",
    "        print(f'{color.bdgreen}History df saved to {workdir}{savename}{color.end}')\n",
    "        del hdf_save\n",
    "        gc.collect()\n",
    "    \n",
    "        print()\n",
    "    \n",
    "        print(\"{}Minimum Validation Loss: {}{:0.4f}{}\".\n",
    "            format(color.bdblue, color.bdgreen, hdf[\"val_loss\"].min(), color.end))\n",
    "        print(\"{}Minimum Loss: {}{:0.4f}{}\".\n",
    "            format(color.bdblue, color.bdgreen, hdf[\"loss\"].min(), color.end))\n",
    "        print(\"{}Maximum Validation Accuracy: {}{:0.4f}{}\".\n",
    "            format(color.bdblue, color.bdgreen, hdf['val_binary_accuracy'].max()*100, color.end))\n",
    "        print(\"{}Maximum Accuracy: {}{:0.4f}{}\".\n",
    "            format(color.bdblue, color.bdgreen, hdf['binary_accuracy'].max()*100, color.end))\n",
    "        print(\"{}Maximum Validation F1-Score: {}{:0.4f}{}\".\n",
    "            format(color.bdblue, color.bdgreen, hdf['val_f1_score'].max()*100, color.end))\n",
    "        print(\"{}Maximum F1-Score: {}{:0.4f}{}\".\n",
    "            format(color.bdblue, color.bdgreen, hdf['f1_score'].max()*100, color.end))\n",
    "        print()\n",
    "    \n",
    "        hdf.loc[:, ['loss', 'val_loss']].plot(title='Cross-entropy Loss')\n",
    "        hdf.loc[:, ['auc', 'val_auc']].plot(title='AUC');\n",
    "        hdf.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title='Accuracy');\n",
    "        hdf.loc[:, ['f1_score', 'val_f1_score']].plot(title='F1-Score');\n",
    "    \n",
    "        return hdf\n",
    "\n",
    "print('KerasBinaryClassification class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tp_class\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>TestPredict Class</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:05.366604Z",
     "iopub.status.busy": "2022-05-23T20:19:05.366176Z",
     "iopub.status.idle": "2022-05-23T20:19:05.384985Z",
     "shell.execute_reply": "2022-05-23T20:19:05.384126Z",
     "shell.execute_reply.started": "2022-05-23T20:19:05.366573Z"
    }
   },
   "outputs": [],
   "source": [
    "class TestPredict:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def PreProcessUserInput(self):\n",
    "        user_input_list = list(self.user_input.values())\n",
    "\n",
    "        # convert to np array\n",
    "        features = np.array([user_input_list])\n",
    "\n",
    "        # since it's only one entry, reshape data so scaling won't return all zeroes\n",
    "        reshapedData = features.reshape(-1,1)\n",
    "\n",
    "        scalar = PowerTransformer(method = 'yeo-johnson')\n",
    "        scalar.fit(reshapedData)\n",
    "        features = scalar.transform(reshapedData)\n",
    "        \n",
    "        # restore to original shape\n",
    "        features = features.reshape(1,-1)\n",
    "    \n",
    "        return features\n",
    "    \n",
    "    # called when predicting score for user input data\n",
    "    def UserInputTestPredict(self,\n",
    "                             testnum = 0,\n",
    "                             user_input = None):\n",
    "        \n",
    "        # user_input is a dictionary; deepcopy() so original is not affected by any changes\n",
    "        self.user_input = copy.deepcopy(user_input)\n",
    "        \n",
    "        print()\n",
    "        print(f\"{color.bold}User Entry: {testnum}{color.end}\")\n",
    "        display(self.user_input)\n",
    "        \n",
    "        features = self.PreProcessUserInput()   # preprocess user input\n",
    "    \n",
    "        pred = (self.model.predict(features) >= 0.5).astype(\"int32\")\n",
    "        \n",
    "        print(f'{color.bdgreen}Prediction: {pred[0][0]}{color.end}')\n",
    "\n",
    "        if pred[0][0] == 1:\n",
    "            print(f'{color.bdblue}Prediction: Approve The Loan{color.end}')\n",
    "        else:\n",
    "            print(f'{color.bdred}Prediction: Do Not Approve The Loan{color.end}')\n",
    "        print()\n",
    "        \n",
    "        # reset self.user_input to default for next User Entry test, if any\n",
    "        self.user_input = copy.deepcopy(user_input)\n",
    "    \n",
    "    # called when predicting score for unseen data (X_test)\n",
    "    def PredictTestData(self,\n",
    "                        X_test = None,\n",
    "                        y_test = None,\n",
    "                        label = \"Keras Model\"):\n",
    "        # Get predictions\n",
    "        print(f'{color.bold}Please wait, preparing predictions{color.end}')\n",
    "\n",
    "        predictions = (self.model.predict(X_test) >= 0.5).astype(\"int32\")\n",
    "\n",
    "        #clear_output(wait=True)\n",
    "        print(f'{color.bdblue}{label} Score:{color.end}')\n",
    "\n",
    "        asm = (accuracy_score(y_test, predictions.round()) * 100)\n",
    "        print(f'{color.bdgreen}Accuracy for model: %.2f %%{color.end}' % asm)\n",
    "        print()    # newline in case the method is run again\n",
    "    \n",
    "print('TestPredict Class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cr_class\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>CompareResults Class</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:05.386793Z",
     "iopub.status.busy": "2022-05-23T20:19:05.386489Z",
     "iopub.status.idle": "2022-05-23T20:19:05.403795Z",
     "shell.execute_reply": "2022-05-23T20:19:05.402869Z",
     "shell.execute_reply.started": "2022-05-23T20:19:05.386755Z"
    }
   },
   "outputs": [],
   "source": [
    "class CompareResults:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def BuildCompare(self,\n",
    "                     hdf1, hdf2,\n",
    "                     col1name, col2name, titlename\n",
    "                    ):\n",
    "    \n",
    "        data = {col1name:[\n",
    "                            hdf1[\"val_loss\"].min(),\n",
    "                            hdf1[\"val_binary_accuracy\"].max()*100,\n",
    "                            hdf1[\"val_f1_score\"].max()*100,\n",
    "                            hdf1[\"loss\"].min(),\n",
    "                            hdf1[\"binary_accuracy\"].max()*100,\n",
    "                            hdf1[\"f1_score\"].max()*100\n",
    "                         ],\n",
    "                col2name:[\n",
    "                            hdf2[\"val_loss\"].min(),\n",
    "                            hdf2[\"val_binary_accuracy\"].max()*100,\n",
    "                            hdf2[\"val_f1_score\"].max()*100,\n",
    "                            hdf2[\"loss\"].min(),\n",
    "                            hdf2[\"binary_accuracy\"].max()*100,\n",
    "                            hdf2[\"f1_score\"].max()*100\n",
    "                         ]\n",
    "                }\n",
    "    \n",
    "        # Create pandas DataFrame\n",
    "        df = pd.DataFrame(data, index = [\n",
    "                                    'Validation Loss',\n",
    "                                    'Validation Binary Accuracy',\n",
    "                                    'Validation F1-Score',\n",
    "                                    'Loss',\n",
    "                                    'Binary Accuracy',\n",
    "                                    'F1-Score'\n",
    "                                        ]\n",
    "                         )\n",
    "        print(f'{color.bdblue}{titlename}{color.end}')\n",
    "        display(df)\n",
    "\n",
    "        acc_diff = round((hdf2[\"binary_accuracy\"].max()*100) -\\\n",
    "                                   (hdf1[\"binary_accuracy\"].max()*100),2)\n",
    "        print(f'{color.bdgreen}Binary Accuracy Improvement:{color.end}')\n",
    "        print(f'{color.bold}Improved by {color.bdblue}{acc_diff} percentage points{color.end}')\n",
    "\n",
    "print('CompareResults Class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optuna_class\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Optuna Class</b><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:05.405829Z",
     "iopub.status.busy": "2022-05-23T20:19:05.405590Z",
     "iopub.status.idle": "2022-05-23T20:19:05.437198Z",
     "shell.execute_reply": "2022-05-23T20:19:05.436111Z",
     "shell.execute_reply.started": "2022-05-23T20:19:05.405804Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class optuna_tuning(KerasBinaryClassification):  \n",
    "    def __init__(self, X, y):\n",
    "        # KerasBinaryClassification will in turn use class PrepDataset's __init__ variables\n",
    "        KerasBinaryClassification.__init__(self, X, y)\n",
    "\n",
    "        self.input_shape = None\n",
    "        self.nn_trials = None\n",
    "        self.nn_timeout = None        # 60 seconds * 120 = 2 hours\n",
    "        self.gt = None                # just for calculating elapsed optuna training time\n",
    "        self.hdg1 = None\n",
    "        self.old_time = dt.datetime.now()\n",
    "        self.current_time = self.old_time.strftime(\"%H:%M:%S\")\n",
    "        self.end_time = None\n",
    "    \n",
    "    # for printing only the best values\n",
    "    def logging_callback(self, study, frozen_trial):\n",
    "        previous_best_value = study.user_attrs.get(\"previous_best_value\", None)\n",
    "        if previous_best_value != study.best_value:\n",
    "            study.set_user_attr(\"previous_best_value\", study.best_value)\n",
    "            clear_output(wait=True)\n",
    "            print(self.hdg1)\n",
    "            print()        \n",
    "            self.text_out=\\\n",
    "                \"{}Trial {} done with best value: {}{}{} and parameters: {}{}{}. \".format(\n",
    "                    color.bdblue,\n",
    "                    frozen_trial.number,\n",
    "                    color.bdgreen,\n",
    "                    frozen_trial.value,\n",
    "                    color.bdblue,\n",
    "                    color.green,\n",
    "                    frozen_trial.params,\n",
    "                    color.end\n",
    "                    )\n",
    "            print(self.text_out)\n",
    "            \n",
    "            # Writing to file\n",
    "            with open(f\"{workdir}ks_optuna_study_log.txt\", \"a\") as os_log:\n",
    "                os_log.write('\\n\\n')\n",
    "                os_log.write(f\"Current Ram Used: {GetRam()} %\\n\")\n",
    "                os_log.write(self.text_out)\n",
    "    \n",
    "    # save study\n",
    "    def save_study(self, study, frozen_trial):\n",
    "        joblib.dump(study, f\"{workdir}ks_optuna_study_callbacks.pkl\")   # save study\n",
    "\n",
    "    # define the model\n",
    "    def create_model(self, trial):\n",
    "        self.n_layers = trial.suggest_categorical(\"n_layers\", [6, 8, 10])\n",
    "        self.UNITS = trial.suggest_categorical(\"units\",\n",
    "                                            [512, 1024, 2048])\n",
    "        self.ACTIVATION = trial.suggest_categorical(\"activation\",\n",
    "                                            [\"relu\", \"swish\"])\n",
    "        self.DROPOUT = trial.suggest_categorical(\"dropout_rate\",\n",
    "                                            [0.1, 0.3, 0.5])\n",
    "       \n",
    "        self.create_func_api_model()   # create model with Functional API method\n",
    "            \n",
    "        # We wont' work on the learning_rate with Optuna, it will be handled by the lr scheduler\n",
    "\n",
    "        # Prepare Keras model (compile, callbacks for early_stopping and lr_schedule)\n",
    "        #self.PrepKSModel()\n",
    "        \n",
    "        print()\n",
    "        param_info =\\\n",
    "            \"{}Current Trial: {}Layers: {} | Units: {} | ACTIVATION: {} | DROPOUT: {}{}\".format(\n",
    "                color.bold,\n",
    "                color.bdblue,\n",
    "                self.n_layers,\n",
    "                self.UNITS,\n",
    "                self.ACTIVATION,\n",
    "                self.DROPOUT,\n",
    "                color.end\n",
    "                )\n",
    "        print(param_info)\n",
    "\n",
    "    # Optuna objective function \n",
    "    def objective_keras(self, trial):\n",
    "        # Clear clutter from previous session graphs.\n",
    "        keras.backend.clear_session()\n",
    "        \n",
    "        # clear outputs, reprint saved data\n",
    "        if trial.number > 0:\n",
    "            clear_output(wait=True)\n",
    "            gc.collect()\n",
    "            print(f\"\\n{color.bold}Time Started = {color.bdgreen}{self.current_time}{color.end}\")\n",
    "            if self.end_time != None:\n",
    "               print(f'{color.bold}Optuna Trials will end around: {color.bdred}{self.end_time}\\\n",
    "                   {color.end}\\n') \n",
    "            print(self.hdg1)\n",
    "            print()\n",
    "            print(self.text_out)\n",
    "        else:\n",
    "            print(f\"\\n{color.bold}Time Started = {color.bdgreen}{self.current_time}{color.end}\")\n",
    "            if self.nn_timeout == None:\n",
    "                self.hdg1 =\\\n",
    "                    \"{}Number of Trials: {} | Epochs: {} | Early Stopping Rounds: {}{}\".format(\n",
    "                        color.bdgreen,\n",
    "                        self.nn_trials,\n",
    "                        self.EPOCHS,\n",
    "                        self.EARLY_STOPPING_ROUNDS,\n",
    "                        color.end\n",
    "                        )\n",
    "            else:\n",
    "                self.hdg1 =\\\n",
    "                    \"{}TimeOut: {} | Epochs: {} | Early Stopping Rounds: {}{}\".format(\n",
    "                        color.bdgreen,\n",
    "                        self.nn_timeout,\n",
    "                        self.EPOCHS,\n",
    "                        self.EARLY_STOPPING_ROUNDS,\n",
    "                        color.end\n",
    "                        )\n",
    "                self.end_time = self.old_time + dt.timedelta(seconds = self.nn_timeout)\n",
    "                print(f'{color.bold}Optuna Trials will end around: {color.bdred}{self.end_time}\\\n",
    "                    {color.end}\\n')\n",
    "            print(self.hdg1)\n",
    "            \n",
    "        # Generate our trial model\n",
    "        self.create_model(trial)\n",
    "    \n",
    "        print()\n",
    "        print(f\"Current Ram Used: {GetRam()} %\")\n",
    "        rt2=dt.datetime.now()\n",
    "        print(f'{color.bdgreen}Total Elapsed Time from Training Start: {color.end}', end='')\n",
    "        runtime(self.gt, rt2)  \n",
    "        print()\n",
    "        print(f'Running Trial {trial.number}')\n",
    "        print()\n",
    "        \n",
    "        # Fit the model on the training data.\n",
    "        # The KerasPruningCallback checks for pruning condition every epoch.\n",
    "        #************************************************************************************  \n",
    "        \n",
    "        history = self.FitModel()\n",
    "        \n",
    "        #************************************************************************************\n",
    "\n",
    "        predictions = (self.model.predict(self.X_valid) >= 0.5).astype(\"int32\")\n",
    "\n",
    "        asm = (accuracy_score(self.y_valid, predictions.round()) * 100)\n",
    "        trial.report(asm, 1)\n",
    "            \n",
    "        #rmse = metrics.mean_squared_error(self.y_valid, predictions,squared=False)\n",
    "        #trial.report(rmse, 1)\n",
    "            \n",
    "        if trial.should_prune():\n",
    "            text_prune = f'{color.bold}Trial {trial.number} pruned{color.end}'\n",
    "            # Writing to file\n",
    "            with open(f\"{workdir}ks_optuna_study_log.txt\", \"a\") as os_log:\n",
    "                os_log.write('\\n')\n",
    "                os_log.write(text_prune)\n",
    "                \n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        text_dtl = \"Trial {} finished with parameters: {}. \".format(\n",
    "            trial.number,\n",
    "            trial.params\n",
    "            )\n",
    "        # Writing to file\n",
    "        with open(f\"{workdir}ks_optuna_study_log.txt\", \"a\") as os_log:\n",
    "            os_log.write('\\n\\n')\n",
    "            os_log.write(f\"Current Ram Used: {GetRam()} %\\n\")\n",
    "            os_log.write(text_dtl)\n",
    "            \n",
    "        rt2=dt.datetime.now()     \n",
    "         \n",
    "        return asm\n",
    "\n",
    "print('optuna_tuning class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"keras_tuner_class\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Keras Tuner Class</b><br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:05.439157Z",
     "iopub.status.busy": "2022-05-23T20:19:05.438603Z",
     "iopub.status.idle": "2022-05-23T20:19:05.453598Z",
     "shell.execute_reply": "2022-05-23T20:19:05.452788Z",
     "shell.execute_reply.started": "2022-05-23T20:19:05.439117Z"
    }
   },
   "outputs": [],
   "source": [
    "class keras_tuning(KerasBinaryClassification):  \n",
    "    def __init__(self, X, y):\n",
    "        # KerasBinaryClassification will in turn use class PrepDataset's __init__ variables\n",
    "        KerasBinaryClassification.__init__(self, X, y)\n",
    "\n",
    "        self.input_shape = None\n",
    "        self.include_lr = False\n",
    "    \n",
    "    # define the model\n",
    "    def model_builder(self, hp):\n",
    "        '''\n",
    "        Args:\n",
    "        hp - Keras tuner object\n",
    "        '''\n",
    "        self.n_layers = hp.Choice('n_layers', values = [6, 8, 10]) \n",
    "        self.UNITS = hp.Choice('units', values = [512, 1024, 2048])\n",
    "        self.ACTIVATION = hp.Choice(\"activation\", [\"relu\", \"swish\"])\n",
    "        self.DROPOUT = hp.Choice(\"dropout_rate\", [0.1, 0.2, 0.3])\n",
    "    \n",
    "        if self.include_lr == True:\n",
    "            # Tune the learning rate for the optimizer\n",
    "            # Choose an optimal value from 0.001, 0.0001, or 0.00001\n",
    "            self.learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "       \n",
    "        self.create_func_api_model()   # create model with Functional API method\n",
    "        \n",
    "        print()\n",
    "        param_info = \"{}Layers: {} | Units: {} | ACTIVATION: {} | DROPOUT: {} | LR: {}{}\".format(\n",
    "           color.bdgreen,\n",
    "            self.n_layers,\n",
    "            self.UNITS,\n",
    "            self.ACTIVATION,\n",
    "            self.DROPOUT,\n",
    "            self.learning_rate,\n",
    "            color.end\n",
    "            )\n",
    "        print(param_info)\n",
    "        \n",
    "        return self.model\n",
    "\n",
    "print('keras_tuning class defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part1\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkRed;color:AliceBlue;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "<h1 style='color:GhostWhite;'>Part 1. tf.Keras Models</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"functions\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Detect and Initialize TPU</b><br>\n",
    "Kaggle's Tensor Processing Units (TPUs) are accelerators ideal for large workloads.  Using a TPU is a bit like using eight GPUs at once.<br><br>\n",
    "For a 10 minute run with TPU active on Kaggle, it will take more than 24 hours without TPU. GPU is still fine, but for the Optuna run, better use TPU.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:05.456478Z",
     "iopub.status.busy": "2022-05-23T20:19:05.454975Z",
     "iopub.status.idle": "2022-05-23T20:19:20.815617Z",
     "shell.execute_reply": "2022-05-23T20:19:20.814777Z",
     "shell.execute_reply.started": "2022-05-23T20:19:05.456434Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tpu_strategy = InitTPUStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:20.817341Z",
     "iopub.status.busy": "2022-05-23T20:19:20.817054Z",
     "iopub.status.idle": "2022-05-23T20:19:20.825043Z",
     "shell.execute_reply": "2022-05-23T20:19:20.824100Z",
     "shell.execute_reply.started": "2022-05-23T20:19:20.817313Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setup plotting\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# Set Matplotlib defaults\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('animation', html='html5')\n",
    "\n",
    "print('Plotting defaults set up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"shared_functions\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Shared Functions</b><br>\n",
    "    - used by different training processes further below\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:20.826850Z",
     "iopub.status.busy": "2022-05-23T20:19:20.826324Z",
     "iopub.status.idle": "2022-05-23T20:19:20.836249Z",
     "shell.execute_reply": "2022-05-23T20:19:20.835549Z",
     "shell.execute_reply.started": "2022-05-23T20:19:20.826794Z"
    }
   },
   "outputs": [],
   "source": [
    "def PredictUnseen(model_name = ''):\n",
    "    ptd = TestPredict(model)\n",
    "    ptd.PredictTestData(\n",
    "                    X_test = dl.X_test,\n",
    "                    y_test = dl.y_test,\n",
    "                    label = f'Unseen Test Data : {model_name}'\n",
    "                    )\n",
    "    \n",
    "print('PredictUnseen() defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:20.838041Z",
     "iopub.status.busy": "2022-05-23T20:19:20.837743Z",
     "iopub.status.idle": "2022-05-23T20:19:20.853265Z",
     "shell.execute_reply": "2022-05-23T20:19:20.852381Z",
     "shell.execute_reply.started": "2022-05-23T20:19:20.838003Z"
    }
   },
   "outputs": [],
   "source": [
    "def UserInputTest():\n",
    "    uit = TestPredict(model)\n",
    "\n",
    "    '''\n",
    "    Let's assume the user inputs client data for a company looking for a loan, then it is processed \n",
    "    by the program to output the user_input dictionary below, which is then used by the model for \n",
    "    prediction.\n",
    "\n",
    "    These user inputs are actually extracted from the unseen X_test dataset.\n",
    "    '''\n",
    "\n",
    "    user_input = {  \n",
    "                'Term':84,\n",
    "                'NoEmp':39,\n",
    "                'NewExist':1,\n",
    "                'CreateJob':0 ,          \n",
    "                'RetainedJob':0,         \n",
    "                'FranchiseCode':1,       \n",
    "                'UrbanRural':1,           \n",
    "                'LowDoc':0,               \n",
    "                'DisbursementGross':81039,                 \n",
    "                'SBA_Appv':25000,          \n",
    "                'Industry':33, \n",
    "                'Recession':0,\n",
    "                'RealEstate':0,           \n",
    "                'SBA_Portion':50,\n",
    "                'State_hash':181805,\n",
    "                'CityState_hash':203332\n",
    "                }      \n",
    "    # Test 1\n",
    "    # prediction is expected to be 1 : approve the loan\n",
    "    uit.UserInputTestPredict(testnum = 1, user_input = user_input)\n",
    "\n",
    "\n",
    "    user_input = {  \n",
    "                'Term':24, \n",
    "                'NoEmp':4,\n",
    "                'NewExist':1,\n",
    "                'CreateJob':0,          \n",
    "                'RetainedJob':0,         \n",
    "                'FranchiseCode':1,       \n",
    "                'UrbanRural':1,           \n",
    "                'LowDoc':0,               \n",
    "                'DisbursementGross':50000,                 \n",
    "                'SBA_Appv':25000,          \n",
    "                'Industry':42, \n",
    "                'Recession':0,\n",
    "                'RealEstate':0,           \n",
    "                'SBA_Portion':50,\n",
    "                'State_hash':321147,\n",
    "                'CityState_hash':699085\n",
    "                }\n",
    "    # Test 2\n",
    "    # prediction is expected to be 0 : do not approve the loan\n",
    "    uit.UserInputTestPredict(testnum = 2, user_input = user_input)\n",
    "    \n",
    "print('UserInputTest() defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"binary_classification\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>1.1 Binary Classification tf.keras model</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we attempt to create a <b>relatively high capacity network</b>, both <b>wide</b> and <b>deep</b>.  \n",
    "\n",
    "\"A network's capacity refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity; and vice versa when overfitting.\n",
    "\n",
    "Capacity can be increased either by making it wider (more units to existing layers) or by making it deeper (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.\"\n",
    "\n",
    "Here we use more units (wide) and more layers (deep) to simulate a wide and deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinaryClassificationModel():\n",
    "    tf.keras.backend.clear_session()\n",
    " \n",
    "    # Process Data\n",
    "    X = pd.read_feather(final_ds)\n",
    "    #X = X.head(1000)      # uncomment for testing\n",
    "    y = X.pop('MIS_Status')\n",
    "    \n",
    "    # Initiate the custom class, this includes initializing most parameters/variables\n",
    "    dl = KerasBinaryClassification(X,y)\n",
    "    dl.osample()                # oversample\n",
    "    dl.split_data()             # split into three - train, valid, test\n",
    "    dl.Scale_Data_Yeo_Johnson()\n",
    "\n",
    "    dl.X, dl.y = None, None\n",
    "    del X, y\n",
    "    gc.collect();\n",
    "\n",
    "    dl.input_shape = [dl.X_train.shape[1]]\n",
    "    print()\n",
    "    print(f'X_train shape: {dl.X_train.shape}')\n",
    "    print(f\"Input shape: {dl.input_shape}\")  \n",
    "    \n",
    "    # Initialize the number of EPOCHS and early stopping value\n",
    "    dl.EPOCHS = 200\n",
    "    dl.EARLY_STOPPING_ROUNDS = dl.EPOCHS * 0.1   # 10%\n",
    "    \n",
    "    dl.n_layers = 6                  # how deep the model will be\n",
    "    \n",
    "    print(f'\\n{color.bdblue}Parameters In Use{color.end}')\n",
    "    param_info = \"{}Layers: {} | Units: {} | ACTIVATION: {} | DROPOUT: {}{}\".format(\n",
    "            color.bdgreen,\n",
    "            dl.n_layers,\n",
    "            dl.UNITS,\n",
    "            dl.ACTIVATION,\n",
    "            dl.DROPOUT,\n",
    "            color.end\n",
    "            )\n",
    "    print(param_info)\n",
    "    \n",
    "    #dl.create_seq_model()            # this creates a model using the Sequential method\n",
    "    \n",
    "    dl.create_func_api_model()       # this creates a model using the Functional API method\n",
    "    \n",
    "    print()\n",
    "    history = dl.FitModel()          # fit the model\n",
    "    \n",
    "    # Save model in Tensorflow's \"SavedModel\" format\n",
    "    save_locally = tf.saved_model.SaveOptions(experimental_io_device = '/job:localhost')\n",
    "    dl.model.save(f'{workdir}ks_bc_model', options = save_locally)\n",
    "\n",
    "    # Zip model directory.  Only doing this to be able to easily download from Kaggle working dir.\n",
    "    ZipDir(f'{workdir}ks_bc_model')\n",
    "    \n",
    "    print()\n",
    "    print(f'{color.bdblue}Model has been saved to {workdir}ks_bc_model{color.end}')\n",
    "    \n",
    "    # Plot model structures\n",
    "    print()\n",
    "    print(f'{color.bdblue}Model structure{color.end}')\n",
    "    display(tf.keras.utils.plot_model(dl.model,\\\n",
    "                              f'{workdir}ks_bc_model.png',show_shapes=True))\n",
    "    \n",
    "    print()\n",
    "    print(f'{color.bdblue}Model Summaries:{color.end}')\n",
    "    display(dl.model.summary())\n",
    "\n",
    "    # Plot Learning Rate history.  Comment out if not using CyclicLR() callback\n",
    "    h = dl.clr_triangular.history\n",
    "    lr = h['lr']\n",
    "    #print(f'lr History: {lr}')\n",
    "    plt.title('Learning Rate History')\n",
    "    plt.plot(dl.clr_triangular.history['iterations'], dl.clr_triangular.history['lr'])\n",
    "  \n",
    "    # Save and Plot training history\n",
    "    hdf = pd.DataFrame(history.history)    \n",
    "    hdf = dl.SavePlotHistory(hdf, 'hdf_ks_bc_model.csv.feather')\n",
    "    \n",
    "    # initialize return value\n",
    "    bc_dict = { 'ks_bc_model':dl.model,\n",
    "                'ks_history':hdf,\n",
    "                'dl_class':dl}\n",
    "\n",
    "    # return value\n",
    "    return bc_dict\n",
    "\n",
    "print('BinaryClassificationModel() defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bc_dict = BinaryClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load model in Tensorflow's \"SavedModel\" format\n",
    "#load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
    "#bc_model = tf.keras.models.load_model(f'{workdir}ks_bc_model', options=load_locally) \n",
    "#model_weights = bc_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bc_dict['ks_bc_model']\n",
    "dl = bc_dict['dl_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test_unseen\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Test Binary Classification Model with Unseen Dataset</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictUnseen(model_name = 'Keras Binary Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"uit\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Test Binary Classification Model with Single User Inputs</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserInputTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if needed\n",
    "#del bc_dict, model, dl\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"binary_classification_wide_and_deep\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>1.2 Binary Classification Wide and Deep tf.keras model</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "A <b>Wide and Deep</b> network trains a linear layer side-by-side with a deep stack of dense layers. Wide and Deep networks are often effective on tabular datasets.\n",
    "\n",
    "Here we use the <b>keras.experimental.WideDeepModel</b> algorithm.  However, in comparison to the previous 1.1 Binary Classification tf.keras model, which is \"wide\" in having an increased capacity with more units to existing layers, and \"deep\" with more layers, the resulting scores are more or less similar.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The parameter best_trial and tuner is set to None by default.\n",
    "tuner:\n",
    "   op : Optuna\n",
    "   kt : Keras Tuner\n",
    "When Optuna or Keras Tuner is run, they will output a best_trial dictionary. This dictionary \n",
    "will then be passed to WideAndDeepModel() to test the Optuna best_trial hyperparameter set.\n",
    "'''\n",
    "def WideAndDeepModel(tuner_type = None, best_trial = None):\n",
    "    tf.keras.backend.clear_session()\n",
    "    X = pd.read_feather(final_ds)\n",
    "    #X = X.head(1000)      # uncomment for testing\n",
    "    y = X.pop('MIS_Status')\n",
    "\n",
    "    # initiate the custom class, this includes initializing most parameters/variables\n",
    "    dl = KerasBinaryClassification(X,y)\n",
    "    dl.osample()\n",
    "    dl.split_data()\n",
    "    dl.Scale_Data_Yeo_Johnson()\n",
    "    \n",
    "    dl.X, dl.y = None, None\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    dl.input_shape = [dl.X_train.shape[1]]\n",
    "    print()\n",
    "    print(f'X_train shape: {dl.X_train.shape}')\n",
    "    print(\"Input shape: {}\".format(dl.input_shape))  \n",
    " \n",
    "    '''Initialize the number of EPOCHS and early stopping value'''\n",
    "    dl.EPOCHS = 200\n",
    "    dl.EARLY_STOPPING_ROUNDS = dl.EPOCHS * 0.1   # 10%\n",
    "      \n",
    "    dl.n_layers = 6\n",
    "\n",
    "    # if WideAndDeepModel() is called when runing Optuna or Keras Tuner\n",
    "    if tuner_type != None:\n",
    "        print('{}{} best_trial parameters have been set for a WideAndDeep model{}'.format(\n",
    "                    color.bdblue,\n",
    "                    tuner_type,\n",
    "                    color.end\n",
    "                    )\n",
    "             )\n",
    "        dl.n_layers = best_trial['n_layers']\n",
    "        dl.UNITS = best_trial['units']\n",
    "        dl.ACTIVATION = best_trial['activation']\n",
    "        dl.DROPOUT = best_trial['dropout_rate']\n",
    "        if 'learning_rate' in best_trial:\n",
    "            dl.kt_run = True   # if True, no learning_rate schedule callback in fit()\n",
    "            dl.learning_rate = best_trial['learning_rate']\n",
    "            lr = dl.learning_rate\n",
    "        else:\n",
    "            lr = 'Not Included'\n",
    "            \n",
    "        param_info =\\\n",
    "            \"{}Layers: {} | Units: {} | ACTIVATION: {} | DROPOUT: {} | LR: {}{}\".format(\n",
    "                    color.bdgreen,\n",
    "                    dl.n_layers,\n",
    "                    dl.UNITS,\n",
    "                    dl.ACTIVATION,\n",
    "                    dl.DROPOUT,\n",
    "                    lr,\n",
    "                    color.end\n",
    "                    )\n",
    "    else:\n",
    "        print(f'{color.bdblue}Parameters In Use{color.end}')\n",
    "        param_info = \"{}Layers: {} | Units: {} | ACTIVATION: {} | DROPOUT: {}{}\".format(\n",
    "            color.bdgreen,\n",
    "            dl.n_layers,\n",
    "            dl.UNITS,\n",
    "            dl.ACTIVATION,\n",
    "            dl.DROPOUT,\n",
    "            color.end\n",
    "            )\n",
    "    \n",
    "    print(param_info)\n",
    "    \n",
    "    dl.create_wide_and_deep_model()\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    history = dl.FitModel()   # fit the model\n",
    "        \n",
    "    # Save in Tensorflow's \"SavedModel\" format\n",
    "    save_locally = tf.saved_model.SaveOptions(experimental_io_device = '/job:localhost')\n",
    "\n",
    "    dl.wide.save(f'{workdir}ks_wide_model', options = save_locally)\n",
    "    dl.deep.save(f'{workdir}ks_deep_model', options = save_locally)\n",
    "    dl.wide_and_deep.save(f'{workdir}ks_wide_and_deep_model', options = save_locally)\n",
    "\n",
    "    # Zip model directory.  \n",
    "    #Only doing this to be able to easily download from Kaggle working dir.\n",
    "    ZipDir(f'{workdir}ks_wide_model')\n",
    "    ZipDir(f'{workdir}ks_deep_model')\n",
    "    ZipDir(f'{workdir}ks_wide_and_deep_model')\n",
    "    \n",
    "    print()\n",
    "    print(f'{color.bdblue}Models have been saved to directory {workdir}{color.end}')\n",
    "  \n",
    "    # Plot model structures\n",
    "    print()\n",
    "    print(f'{color.bdblue}Model structure{color.end}')\n",
    "    print(f'{color.bdgreen}Wide model{color.end}')\n",
    "    display(tf.keras.utils.plot_model(dl.wide,\\\n",
    "                              f'{workdir}ks_wide_model.png',show_shapes=True))\n",
    "    print(f'{color.bdgreen}Deep model{color.end}')\n",
    "    display(tf.keras.utils.plot_model(dl.deep,\\\n",
    "                              f'{workdir}ks_deep_model.png',show_shapes=True))\n",
    "    print(f'{color.bdgreen}Wide and Deep model{color.end}')\n",
    "    display(tf.keras.utils.plot_model(dl.wide_and_deep,\\\n",
    "                              f'{workdir}ks_wide_and_deep_model.png',show_shapes=True))\n",
    "    \n",
    "    print()\n",
    "    print(f'{color.bdblue}Model Summaries:{color.end}')\n",
    "    display(dl.wide_and_deep.summary())\n",
    "    print()\n",
    "    display(dl.wide.summary())\n",
    "    print()\n",
    "    display(dl.deep.summary())\n",
    "    print()\n",
    "\n",
    "    # Save and Plot history\n",
    "    hdf = pd.DataFrame(history.history)\n",
    "    if tuner_type == None:\n",
    "        hdf_savename = 'hdf_ks_wd_model.csv.feather'\n",
    "        #print(f'{color.bdgreen}Wide And Deep History saved to {hdf_savename}{color.end}')\n",
    "    else:\n",
    "        if tuner_type == 'op':\n",
    "            hdf_savename = 'hdf_optuna_ks_wd_model.csv.feather'\n",
    "        else:\n",
    "            hdf_savename = 'hdf_keras_tuner_ks_wd_model.csv.feather'\n",
    "        #print(f'{color.bdgreen}Wide And Deep Optuna History saved to {hdf_savename}{color.end}')\n",
    "    hdf = dl.SavePlotHistory(hdf, hdf_savename)\n",
    "\n",
    "    # initialize return value\n",
    "    wd_dict = { 'ks_wide_and_deep_model':dl.wide_and_deep,\n",
    "                'ks_deep_model':dl.deep,\n",
    "                'ks_wide_model':dl.wide,\n",
    "                'ks_history':hdf,\n",
    "                'dl_class':dl}\n",
    "    return wd_dict\n",
    "\n",
    "print('WideAndDeepModel defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wd_dict = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wd_dict['ks_wide_and_deep_model']\n",
    "dl = wd_dict['dl_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.metrics_names)\n",
    "display(model.loss)\n",
    "display(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test_unseen_wd\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Test keras.experimental.WideDeepModel with Unseen Dataset</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictUnseen(model_name = 'keras.experimental.WideDeepModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"uit_wd\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Test keras.experimental.WideDeepModel with Single User Inputs</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function should have been defined earlier before running\n",
    "\n",
    "UserInputTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if needed\n",
    "#del wd_dict, model, dl\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compare_bc_wd\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>1.3 Score Comparison : Binary Classification vs WideAndDeep Models</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareModels():\n",
    "    hdf1 = pd.read_feather(f'{workdir}hdf_ks_bc_model.csv.feather')\n",
    "    hdf2 = pd.read_feather(f'{workdir}hdf_ks_wd_model.csv.feather')\n",
    "    \n",
    "    crc = CompareResults()  # instantiate custom class\n",
    "\n",
    "    crc.BuildCompare(hdf1, hdf2,\n",
    "                 'Binary Classification',     # column 1 name\n",
    "                 'WideAndDeep',               # column 2 name\n",
    "                 'Standard Binary Classification vs WideAndDeep Scores'      # title\n",
    "                )\n",
    "    \n",
    "CompareModels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The scores between using the simulated wide and deep Binary Classification model versus the keras.experimental.WideDeepModel model are <b>very similar</b>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part2\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkRed;color:AliceBlue;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "<h1 id=\"part2\" style='color:GhostWhite;'>Part 2. Optuna Hyperparameter Tuning\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optuna_study\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>2.1 Optuna Study</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.integration import KerasPruningCallback\n",
    "# For running Optuna tuning on full dataset.\n",
    "def OptunaStudyKeras():\n",
    "    tf.keras.backend.clear_session()\n",
    "    X = pd.read_feather(final_ds)\n",
    "    #X = X.head(1000)      # uncomment for testing\n",
    "    y = X.pop('MIS_Status')\n",
    "\n",
    "    features_num = list(X)\n",
    "    print(features_num)\n",
    "    \n",
    "    # initiate the custom class\n",
    "    dl = optuna_tuning(X,y)\n",
    "    dl.osample()\n",
    "    dl.split_data()\n",
    "    dl.Scale_Data_Yeo_Johnson()\n",
    " \n",
    "    dl.X, dl.y = None, None\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    dl.input_shape = [dl.X_train.shape[1]]\n",
    "    print()\n",
    "    print(\"Input shape: {}\".format(dl.input_shape))  \n",
    "\n",
    "    if os.path.exists(f'{workdir}ks_optuna_study_log.txt'):\n",
    "        os.remove(f'{workdir}ks_optuna_study_log.txt')\n",
    "    \n",
    "    #*********************************************************\n",
    "    # Initialize number of trials and epochs\n",
    "    dl.nn_trials = None         \n",
    "    dl.nn_timeout = 60 * 240    # 3 hours  \n",
    "    dl.EPOCHS = 200\n",
    "    dl.EARLY_STOPPING_ROUNDS = dl.EPOCHS * 0.1   # 10%\n",
    "    #*********************************************************\n",
    "    \n",
    "    # STUDY\n",
    "    \n",
    "    # Turn off optuna log notes, to use own logging notes\n",
    "    #optuna.logging.set_verbosity(optuna.logging.WARN)\n",
    "    optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "    print(f\"{color.bold}Please wait, finding best trial ...{color.end}\")         \n",
    "    dl.gt = dt.datetime.now()   # to show total runtime later\n",
    "    \n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    try:\n",
    "        # callbacks [self.save_study] is to save study in case memory fails\n",
    "        study.optimize(dl.objective_keras,\n",
    "                        n_trials = dl.nn_trials,\n",
    "                        timeout = dl.nn_timeout,\n",
    "                        callbacks = [dl.logging_callback, dl.save_study],\n",
    "                        gc_after_trial = True,\n",
    "                        catch = (RuntimeWarning,ArithmeticError,))\n",
    "    except MemoryError as e:\n",
    "        print(f'{color.bdblue}{e} : Memory was getting low, Trial ended early{color.end}')\n",
    "        \n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "    print()\n",
    "    print(f'Number of finished trials: {len(study.trials)}')\n",
    "    print(f'Number of pruned trials: {len(pruned_trials)}')\n",
    "    print(f'Number of completed trials: {color.bdgreen}{len(complete_trials)}{color.end}')\n",
    "    print(f'{color.bdblue}Best trial: {study.best_trial.params}{color.end}')\n",
    "    \n",
    "    joblib.dump(study, f\"{workdir}ks_optuna_study.pkl\")   # save study\n",
    "\n",
    "    print()\n",
    "    return {'study':study}\n",
    "\n",
    "print('OptunaStudyKeras() has been defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if optuna_flag == 1:\n",
    "    study_results = OptunaStudyKeras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optuna_metrics\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>2.2 Optuna Study Metrics</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the trials log; Very useful to know which parameters one can avoid\n",
    "with open(f\"{workdir}ks_optuna_study_log.txt\", \"r+\") as os_log:\n",
    "    print(os_log.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below if you want to load saved study to check, if desired\n",
    "jl = joblib.load(f\"{workdir}ks_optuna_study.pkl\")\n",
    "jl = jl.best_trial.params\n",
    "display(jl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one can get the best trial by loading the saved study\n",
    "study = joblib.load(f\"{workdir}ks_optuna_study.pkl\")\n",
    "best_trial = study.best_trial.params\n",
    "display(best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, we just use the study returned by OptunaStudyKeras() previously\n",
    "best_trial = study_results['study'].best_trial.params\n",
    "best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial results dataframe sorted from best value (RMSE) ascending\n",
    "def ViewResultsAsDf():\n",
    "    stdf = study_results['study'].trials_dataframe()\n",
    "    stdf = stdf.sort_values('value',ascending=True)\n",
    "\n",
    "    return stdf.head(2)    # return here is only used for printing output\n",
    "\n",
    "ViewResultsAsDf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize parameter importance\n",
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"try_best_hp\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>2.3 Test Optuna Best Trial Hyperparameters</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wd_dict = WideAndDeepModel(tuner_type = 'op', best_trial = best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optuna_comparison\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>2.4 Optuna Tuning Comparison</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Compare metrics before and after Optuna tuning.</b><br><br>\n",
    "    Comparison is made between the WideAndDeep model results before and after Optuna tuning.\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareOptuna():\n",
    "    hdf1 = pd.read_feather(f'{workdir}hdf_ks_wd_model.csv.feather')\n",
    "    hdf2 = pd.read_feather(f'{workdir}hdf_optuna_ks_wd_model.csv.feather')\n",
    "    \n",
    "    crc = CompareResults()  # instantiate custom class\n",
    "\n",
    "    crc.BuildCompare(hdf1, hdf2,\n",
    "                 'Before Optuna',     # column 1 name\n",
    "                 'After Optuna',      # column 2 name\n",
    "                 'Binary Accuracy Improvement Using Optuna Suggested Parameters:'   # title\n",
    "                )\n",
    "    \n",
    "CompareOptuna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    I used <b>4 hours of Optuna training time</b>, with TPU as accelerator, and could have had a better score with a longer time which would mean more trial runs.  We could also repeat the run.  Anyway, this is just to demonstrate Optuna Tuning for Keras.<br><br>\n",
    "Also, it seems that I already chose a good hyperparameter set to start with, before Optuna.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wd_dict['ks_wide_and_deep_model']\n",
    "dl = wd_dict['dl_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optuna_test_unseen_wd\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Test Optuna-Tuned keras.experimental.WideDeepModel with Unseen Dataset</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictUnseen(model_name = 'Optuna-tuned keras.experimental.WideDeepModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optuna_uit\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Test Optuna-Tuned keras.experimental.WideDeepModel with Single User Inputs</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserInputTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if needed\n",
    "#del best_trial, study_results, wd_dict, model, dl\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if alert_flag == 1:\n",
    "    if kaggle_flag == 0:   # not Kaggle\n",
    "        engine.say(\"Optuna run completed.\")\n",
    "        engine.runAndWait()\n",
    "    else:\n",
    "        display(Audio(url=audio_path, autoplay=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part3\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkRed;color:AliceBlue;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "<h1 id=\"part2\" style='color:GhostWhite;'>Part 3. Keras Tuner Hyperparameter Tuning\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "I favor Optuna more than Keras Tuner, especially due to Optuna's speed.  At any rate, below is the code demonstration on how to use Keras Tuner, for reference.  It is also significantly slower than Optuna, even with TPU.<br><br>\n",
    "As at May 2022, Kaggle Tuner as is will fail in Kaggle, as it will be unable to save epoch checkpoints to Kaggle's working folder, it can only save to a Google Cloud Storage (gcs) location.  As a temporary solution, we mock/patch the Kaggle Tuner function that is responsible, and this solves the issue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kt_study\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>3.1 Keras Tuner Search</h2>\n",
    "    Here we use the BayesianOptimization method, which is slower than Optuna.  We could also have used the faster Hyperband method (more like RandomSearch).   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>HyperBand</b><br>\n",
    "\"This technique tries to remove one of the problems in random search of hyperparameters. Problem is as follows: Random search may pick some values which are very obviously bad and will do full training and evaluation on it, which is wasteful. Hyperband provides one way to solve this problem. \n",
    "\n",
    "Hyperband Solution: Randomly sample all the combinations of hyperparameter and now instead of running full training and evaluation on it, train the model for few epochs (less than max_epochs) with these combinations and select the best candidates based on the results on these few epochs. It does this iteratively and finally runs full training and evaluation on the final chosen candidates. The number of iterations done depends on parameter hyperband_iterations and number of epochs in each iteration are less than max_epochs.\"<br><br>\n",
    "<b>Bayesian Optimization</b><br>\n",
    "\"This techniques addresses a common problem in RandomSearch and Hyperband. Problem: All the hyperparameter combinations are chosen randomly. Choosing hyperparameters randomly helps to explore the hyperparameter space but does not guarantee absolute optimal hyperparameters. \n",
    "\n",
    "Bayesian Ozptimization Solution: Instead of all combinations being random, it chooses first few randomly, then based on the performance on these hyperparameters it chooses the next best possible hyperparameters. <b>Hence it takes into account the history of the hyperparameters which were tried.</b> The iterations of choosing next set of hyperparameters based on history and evaluating performance continues till the tuner reaches optimal hyperparameters or exhausts maximum number of allowed trails.\"<br>\n",
    "- <a href=\"https://medium.com/swlh/hyperparameter-tuning-in-keras-tensorflow-2-with-keras-tuner-randomsearch-hyperband-3e212647778f\"><i>from Hyperparameter Tuning in Keras</i></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:31.056331Z",
     "iopub.status.busy": "2022-05-23T20:19:31.056031Z",
     "iopub.status.idle": "2022-05-23T20:19:31.087848Z",
     "shell.execute_reply": "2022-05-23T20:19:31.087009Z",
     "shell.execute_reply.started": "2022-05-23T20:19:31.056303Z"
    }
   },
   "outputs": [],
   "source": [
    "def KerasTuner(tuner_to_use = 'bo'):\n",
    "    tf.keras.backend.clear_session()\n",
    "    X = pd.read_feather(final_ds)\n",
    "    X = X.head(1000)      # uncomment for testing\n",
    "    y = X.pop('MIS_Status')\n",
    "    \n",
    "    # initiate the custom class\n",
    "    dl = keras_tuning(X,y)\n",
    "    dl.osample()\n",
    "    dl.split_data()\n",
    "    dl.Scale_Data_Yeo_Johnson()\n",
    " \n",
    "    dl.X, dl.y = None, None\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    dl.input_shape = [dl.X_train.shape[1]]\n",
    "    print()\n",
    "    print(\"Input shape: {}\".format(dl.input_shape))  \n",
    "\n",
    "    #***********************************************************\n",
    "    # Initialize number of trials and epochs\n",
    "    dl.EPOCHS = 2\n",
    "    dl.EARLY_STOPPING_ROUNDS = dl.EPOCHS * 0.1   # 10%\n",
    "    \n",
    "    '''include learning_rate in tuning.  Personally,\n",
    "    I prefer to use an lr_scheduler, but we include it here\n",
    "    for demonstration purpose'''\n",
    "    dl.include_lr = True\n",
    "                     \n",
    "    if tuner_to_use == 'bo':\n",
    "        nn_trials = 2       # only for BayesianOptimization\n",
    "        \n",
    "    #************************************************************\n",
    "\n",
    "    # TUNER SEARCH\n",
    "\n",
    "    print(f\"{color.bold}Please wait, finding best trial ...{color.end}\")         \n",
    "    \n",
    "    if tuner_to_use == 'bo':\n",
    "        # Instantiate the search\n",
    "        # Here, we use BayesianOptimization.  The hypermodel function builds the model,\n",
    "        # and calls the compile method as well as initialize the early_stopping callback\n",
    "        tuner = kt.BayesianOptimization(\n",
    "            dl.model_builder,                  # the hypermodel, found in custom class keras_tuning \n",
    "            objective = 'val_binary_accuracy', # objective to optimize\n",
    "            max_trials = nn_trials,            # number of trials\n",
    "            distribution_strategy = tpu_strategy,\n",
    "            # this project name (a dir) is created in 'directory', and is used to \n",
    "            #     resume tuning if the search is run again\n",
    "            directory = f'{workdir}',   \n",
    "            project_name = 'kbayesian',\n",
    "            # if True, overwrite above directory if search is run again - i.e. don't resume\n",
    "            overwrite = True\n",
    "        )     \n",
    "    else:    \n",
    "        # Here, we use HyperBand\n",
    "        tuner = kt.Hyperband(\n",
    "            dl.model_builder,\n",
    "            objective = 'val_binary_accuracy',\n",
    "            max_epochs = dl.EPOCHS,\n",
    "            factor = 30000000000,\n",
    "            # One iteration will run approximately\n",
    "            #     max_epochs * (math.log(max_epochs, factor) ** 2) \n",
    "            #     cumulative epochs across all trials\n",
    "            hyperband_iterations = 1,    \n",
    "            distribution_strategy = tpu_strategy,\n",
    "            directory = f'{workdir}',         \n",
    "            project_name = 'khyperband',\n",
    "            overwrite = True\n",
    "         ) \n",
    "\n",
    "    # hypertuning settings\n",
    "    print()\n",
    "    display(tuner.search_space_summary())\n",
    "    \n",
    "    #*****************************************************************************************\n",
    "    '''\n",
    "    When we run the keras-tuner search for hyperparameters with TPU accelerator in Kaggle, we \n",
    "    will get the error: 'UnimplementedError: File system scheme '[local]' not implemented', since\n",
    "    keras-tuner does not have the option to save checkpoints to a locally - i.e. the Kaggle working \n",
    "    directory.\n",
    "    \n",
    "    To fix this, we can use a gcs location under directory parameter, and that's it.\n",
    "    \n",
    "    However, if we still want to save to the Kaggle working directory, then we mock / patch the \n",
    "    keras-tuner function that saves checkpoints, using our own function temporarily to allow \n",
    "    saving locally.  \n",
    "    \n",
    "    First of all, Keras Tuner version must be 1.0.3 and above.\n",
    "    \n",
    "    Then, here we \"replace\" the function on_epoch_end() with new_on_epoch_end().\n",
    "    This function is under keras_tuner.engine.tuner_utils.SaveBestEpoch class.\n",
    "    \n",
    "    We use \"with patch()\", which will temporarily replace the function with our code, and \n",
    "    revert it back automatically after use.\n",
    "    '''\n",
    "    #*****************************************************************************************\n",
    "    def new_on_epoch_end(self, epoch, logs=None):\n",
    "        if not self.objective.has_value(logs):\n",
    "            # Save on every epoch if metric value is not in the logs. Either no\n",
    "            # objective is specified, or objective is computed and returned\n",
    "            # after `fit()`.\n",
    "            \n",
    "            '''we comment out the original code'''\n",
    "            #self.model.save_weights(self.filepath)\n",
    "             \n",
    "            '''we put in our code'''\n",
    "            # Save model in Tensorflow's \"SavedModel\" format\n",
    "            save_locally = tf.saved_model.SaveOptions(experimental_io_device = '/job:localhost')\n",
    "            self.model.save_weights(self.filepath, options = save_locally)\n",
    "\n",
    "            return\n",
    "        current_value = self.objective.get_value(logs)\n",
    "        if self.objective.better_than(current_value, self.best_value):\n",
    "            self.best_value = current_value\n",
    "            \n",
    "            '''we comment out the original code'''\n",
    "            #self.model.save_weights(self.filepath)\n",
    "            \n",
    "            '''we put in our code'''\n",
    "            # Save model in Tensorflow's \"SavedModel\" format\n",
    "            save_locally = tf.saved_model.SaveOptions(experimental_io_device = '/job:localhost')\n",
    "            self.model.save_weights(self.filepath, options = save_locally)    \n",
    "    \n",
    "    '''\n",
    "    with patch() will now make tuner.search() use our patch, and automatically revert back\n",
    "    to original after use\n",
    "    '''\n",
    "    with patch('keras_tuner.engine.tuner_utils.SaveBestEpoch.on_epoch_end', new_on_epoch_end):\n",
    "        # Perform hypertuning.  The parameters are exactly like those in the fit() method.\n",
    "        tuner.search(\n",
    "            dl.X_train,\n",
    "            dl.y_train,\n",
    "            epochs=dl.EPOCHS,\n",
    "            validation_data = (dl.X_valid, dl.y_valid), \n",
    "            callbacks=[dl.early_stopping]   # found at KerasBinaryClassification.PrepKSModel()\n",
    "            )\n",
    "        \n",
    "    return {\n",
    "            'tuner':tuner,\n",
    "            'dl':dl,\n",
    "           }\n",
    "\n",
    "print('KerasTuner() defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T20:19:34.740805Z",
     "iopub.status.busy": "2022-05-23T20:19:34.740527Z",
     "iopub.status.idle": "2022-05-23T20:20:39.713492Z",
     "shell.execute_reply": "2022-05-23T20:20:39.712567Z",
     "shell.execute_reply.started": "2022-05-23T20:19:34.740778Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if kt_flag == 1:\n",
    "    '''\n",
    "    bo = BayesianOptimization\n",
    "    hb = Hyperband\n",
    "    '''\n",
    "    kt_retval = KerasTuner(tuner_to_use = 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kt_metrics\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>3.2 Keras Tuner Metrics</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary(num_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "best_trial = best_hp.values\n",
    "best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kt_try_best_hp\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>3.3 Test Keras Tuner's Best Hyperparameters</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wd_dict = WideAndDeepModel(tuner_type = 'kt', best_trial = best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kt_comparison\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:DarkCyan;color:Azure;text-align: left;padding-top: 5px;padding-bottom: 20px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <h2 style='color:GhostWhite;'>3.4 Keras Tuner Tuning Comparison</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Compare metrics before and after Keras Tuner tuning.</b><br><br>\n",
    "    Comparison is made between the WideAndDeep model results before and after Keras Tuner tuning.\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareKerasTuner():\n",
    "    hdf1 = pd.read_feather(f'{workdir}hdf_ks_wd_model.csv.feather')\n",
    "    hdf2 = pd.read_feather(f'{workdir}hdf_keras_tuner_ks_wd_model.csv.feather')\n",
    "    \n",
    "    crc = CompareResults()  # instantiate custom class\n",
    "\n",
    "    crc.BuildCompare(hdf1, hdf2,\n",
    "                 'Before KT',     # column 1 name\n",
    "                 'After KT',      # column 2 name\n",
    "                 'Binary Accuracy Improvement Using KT Suggested Parameters:'   # title\n",
    "                )\n",
    "\n",
    "CompareKerasTuner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wd_dict['ks_wide_and_deep_model']\n",
    "dl = wd_dict['dl_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kt_test_unseen_wd\"></a>\n",
    "<div style=\"font-family: Trebuchet MS;background-color:LightSteelBlue;color:Black;text-align: left;padding-top: 5px;padding-bottom: 15px;padding-left: 20px;padding-right: 10px;border-radius: 15px 50px;letter-spacing: 2px;\">\n",
    "    <b>Test KT-tuned keras.experimental.WideDeepModel with Unseen Dataset</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredictUnseen(model_name = 'Keras Tuner with keras.experimental.WideDeepModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserInputTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><a style=\"color:DarkSlateGrey\" href=\"#toc\">Back to Table Of Contents</a></i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
